---
title: "Data Science Capstone - Tweet Classification System - Catastrophe or Not?"
author: "Carlos Yáñez Santibáñez"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    latex_engine: pdflatex
    fig_caption: yes
    includes:
      in_header: header_includes.tex
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
geometry: margin=1in,headheight=70pt,headsep=0.3in
linkcolor: blue
mainfont: Arial
fontsize: 11pt
always_allow_html: yes
---

 <!-- To compile the report into pdf, the files header_includes.tex is require. This file is available at -->
 <!-- https://github.com/carlosyanez/Tweets_Catastrophe_Classification/blob/master/header_includes.tex -->
 

```{r, setup, include=FALSE}
#### WARNING : Running the notebook from scratch, may take SEVERAL HOURS - Run at your own time.####
### Please go through the below options carefully, ad it will determine how this report will be generated.

### Options on how to generate this report
###Download data from the external source file (done last time on Sat 11 Apr 2020)
### NOTE: After collation of this report, it was noticed that the source data has been changed - addressing some the issues highlight here. This may alter the results.

download_source_file <- FALSE
### Evaluate all chunks- this may take a lot of time and includes "unshortening" t.co URLs
eval<-FALSE
### Dowload the processed files, to avoid de above
download_processed <- TRUE
download_clean <- FALSE ## select TRUE if you want the "clean" versions, without profane words and anonimised URLs.
### Download .RData file if you just want to knit the report without doing all the calculations (to save time)
download_rdata <- TRUE

# Avoid to run time consuming analysis - comment if you want to run the notebook from scratch

knitr::opts_chunk$set(echo=FALSE,eval=eval,message=FALSE, warning=FALSE,tidy=FALSE,fig.align="center",
                      fig.width=5, fig.height=3)


knitr::knit_hooks$set(inline = function(x) { if(!is.numeric(x)){ x }else{ prettyNum(round(x,3), big.mark=" ") } })

# Load addiional packages for reporting, not included in R file
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(plotly)) install.packages("urltools", repos = "http://cran.us.r-project.org")
if(!require(ggrepel)) install.packages("ggrepel", repos = "http://cran.us.r-project.org")

library(data.table)
library(kableExtra)
library(gridExtra)
library(plotly)
library(ggrepel)


#Dowload Environment file, for knitting.
if(download_rdata==TRUE){
    if(!file.exists("Twitter_challenge.RData")){
    download.file("https://github.com/carlosyanez/Tweets_Catastrophe_Classification/raw/master/Twitter_challenge.RData",
                  "Twitter_challenge.RData")
    }
  load("Twitter_challenge.RData")  
}

# Load functions created for this excercise

if(!file.exists("twitter_classifier.R")){
  download.file("https://raw.githubusercontent.com/carlosyanez/Tweets_Catastrophe_Classification/master/twitter_classifier.R","twitter_classifier.R")
  }
source("twitter_classifier.R", echo = F, prompt.echo = "", spaced = F)

```

\newpage

# Introduction
<!-- An introduction/overview/executive summary section that describes the dataset and variables, and summarizes the goal of the project and key steps that were performed. -->

This document presents a machine learning model used to predict whether a particular tweet (message on [Twitter.com](http://www.twitter.com)) is related to a disaster/catastrophic event. This report is the last capstone assignment for HarvardX's Professional Certificate in Data Science, which can be taken at the edX platform. The program is available at   [https://www.edx.org/professional-certificate/harvardx-data-science]( https://www.edx.org/professional-certificate/harvardx-data-science).

This problem has been inspired in a Kaggle's beginner competition called [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started). However, the data used in this  data used in this exercise does not come from Kaggle. Instead, it has been downloaded directly from Kaggle's inspiration on figure-eight's  ["Data for Everyone"](https://www.figure-eight.com/data-for-everyone/) webpage. 

Based on that aforementioned dataset, this report proposed an machine learning model that divides each tweet into four constituent components. A prediction for each of the component is made and an overall prediction is created based on those individual results. The four components or attributes are:

* The text/words of the tweet (natural language).
* The hashtags in the tweet, which should be meaningful labels for the content.
* The links included in the message. Since links are shared using the t.co shortening services, those will be "unshortened" and classified according their original URL domain.
*  Any mention to other Twitter users ("handles" in Twitter's terminology).

This work is presented in the following sections:

1. This **Introduction**.
2. **Analysis**, which includes: 1. Initial data loading and cleaning into a workable format 2. Initial modelling,  where the prediction algorithm is created. 3.  Algorithm Finalisation, where it is tested and refined 4.  Tuning of the most relevant parameters
3. Evaluation of **results** against validation dataset.
4. **Conclusions**.

The subsequent sections present the above points in further detail.

This file, and its associated R and csv files are available in [Github](https://github.com/carlosyanez/Tweets_Catastrophe_Classification).

\newpage
# Analysis
<!-- A methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, any insights gained, and your modelling approach. At least two different models or algorithms must be used, with at least one being more advanced than simple linear regression for prediction problems. -->

As mentioned in the **Introduction**, this report presents an attempt to build a machine learning model to classify tweets. In this particular case, the question to answer is if those tweets correspond to a catastrophic event or not. The idea for this was taken of a Kaggle Competition for Starters ([Real or Not? NLP with Disaster Tweets]([https://www.kaggle.com/c/nlp-getting-started])). Instead of using the data provided in Kaggle, in this report the original version in [Data For Everyone](https://www.figure-eight.com/data-for-everyone/) is used.

To start, let's have look at a sample of dataset:

```{r, download_source_file}
#download data
if(download_source_file==TRUE){
  dl <- tempfile()
  download.file("https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/socialmedia-disaster-tweets-DFE.csv",
                dl)
  source_data <- read_csv(dl, locale = locale(encoding = "macintosh"))
  rm(dl)

#initial transformation to feed into
  source_data$id <- seq.int(nrow(source_data))
  source_data <- source_data %>% mutate(target=ifelse(choose_one=="Relevant",1,0)) %>% 
                select(id,target,text, keyword,location) 

#process source
  processed_source <-  prepare_data(source_data,profanity_clean = 0) 
  write_csv(processed_source,"processed_source.csv")

  processed_source2 <-  prepare_data(source_data,profanity_clean = 1) 
  write_csv(processed_source2,"processed_source_clean.csv")

  rm(profanity_alvarez,profanity_arr_bad,profanity_banned,profanity_racist,profanity_zac_anger,processed_source2)
}
```

```{r, show_source_data, eval=TRUE}

source_data %>% select(id,target,text=original_text,keyword,location)%>%
  filter(id %in% c(1,6,33,49,160,914,3999,7336)) %>% 
  kable(caption="Sample Data",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  column_spec(3,width = "15em") %>%
  kable_styling(latex_options =c("striped","hold_position","scale_down")) 
  
```

As seen above, the dataset contains the following observations:

* **id** : unique sequential identifier.
* **target** : indicator of whether the tweet corresponds to a catastrophic event (**1**) or not (**0**).
* **text** : tweet, as extracted from Twitter.
* **keyword** : search term used in Twitter's search bar to retrieve the tweets.
* **location** : Name of the place where the message originated.

For this report, **keyword** and **location** will be ignored, choosing to focus on the text (tweet) only. After inspection of its content,  the text will be divided into four components (or attributes):

* The text proper. i.e. the *natural language* sentence in the text. In the report, this will be called **text** or **words**. All the tweets in this file are written in English language, but they may contain non-English characters, emojis and emoticons.
* The **hashtags**, which are  user-created categories. This is a meaningful component of the tweet and may help clarify  whether the message is a real disaster. For example, in the below tweet, the hashtag helps us to elucidate this is cricket commentary rather than a fire event:

```{r, tweet_hasthag_example,eval=TRUE}

source_data %>% filter(id==1674) %>% select(id,text) %>% 
                kable(caption="Tweet about cricket",
                      format = "latex", booktabs = TRUE,
                      format.args = list(decimal.mark = '.',   big.mark = " ")) %>%
                column_spec(2,width = "40em") %>%
                kable_styling(latex_options =c("HOLD_position","scale_down")) 
  
```

* The **links** contained in the tweet. These are presented as a t.co link (Twitter's link sharing service). In this format they are not much information, but it is possible to retrieve the original URLs. This can provide further insight into the nature of the message - e.g. if the user is sharing a link to a disaster management agency or a sports website.
* Who is mentioned in the tweet (by their Twitter **handles**). Similarly to the previous points, it may be useful to distinguish between people reaching out to government/emergency service from *[shock jocks](https://en.wikipedia.org/wiki/Shock_jock)* using florid language.

## Processing and cleaning up the data

In order to obtain the data in a useful format, the function *prepare_data* has been created. Through its code, this functions conducts the following tasks:

1. Extract hashtags, links and Twitter handles from the tweet.
2. Clean up the remaining text, remove links and handles, and "translate"  emojis,emoticons and other special characters.
3. (Optional) Replace swear words and profane language with unique strings, in case their reading needs to be avoided, yet retaining the words for analysis. Please note this report has been generated with the uncensored version, thus results may vary if the "sanitised" version is used.
4. "Unshorten" link URLs to obtain domain names of linked sources.

In this function, the first step is similar for the three components - below is the code use to remove the hashtags:

```{r hasthag_removal_code, echo=TRUE, eval=FALSE}

 source_data$hashtag <- str_extract_all(source_data$text, "#\\S+")
  source_data <- source_data %>% 
    mutate(hashtag = gsub(x=hashtag, 
           pattern = "character\\(0)", replacement = "")) %>%
    mutate(hashtag = gsub(x=hashtag, pattern = "c\\(", replacement = "")) %>%
    mutate(hashtag = gsub(x=hashtag, pattern = "\"", replacement = "")) %>%
    mutate(hashtag = gsub(x=hashtag, pattern = ")", replacement = "")) %>%
    mutate(hashtag = gsub(x=hashtag, pattern = ",", replacement = ""))
  
```


Then both links and mentions are removed, keeping the hashtags since they may also be part of the sentence in question. Then, leveraging the functions in the **[textclean](https://www.rdocumentation.org/packages/textclean/versions/0.9.3)** package, the remaining data is further normalised, by replacing :

* contractions with full words,
* emojis and emoticons with equivalent word (e.g. :) with 'smile'),
* all non-ASCII characters with its equivalent ASCII-compliant character,
* Internet slang with full words,
* html code,
* money symbols,
* numbers with full words,
* ordinals with full words,
* timestamps.

The code that achieves this is shown below:

```{r, text_cleaning, echo=TRUE, eval=FALSE}

 #remove mentions and links from text, save original text in different observation
  
   source_data$original_text <- source_data$text
   source_data <- source_data %>% 
    mutate( text = gsub(x = text, pattern = "#", replacement = ""))  %>%
    mutate( text = gsub(x = text, pattern = "@\\S+", replacement = "")) %>%
    mutate(text = gsub(x=text, 
                       pattern = "(s?)(f|ht)tp(s?)://\\S+\\b", 
                       replacement = "")) 
  
#further clean text with textclean package
  
  source_data$text<-replace_contraction(source_data$text)
  source_data$text<- replace_emoji(source_data$text)
  source_data$text<- replace_emoticon(source_data$text)
  source_data$text<-replace_non_ascii(source_data$text,impart.meaning=TRUE)
  source_data$text<-replace_internet_slang(source_data$text)
  source_data$text<-replace_html(source_data$text)
  source_data$text<-replace_money(source_data$text)
  source_data$text<-replace_number(source_data$text)
  source_data$text<-replace_ordinal(source_data$text)
  source_data$text<-replace_time(source_data$text)
  
```

An optional step is to remove swear and profane vocabulary from the text, in case that is desired. This has been done through the below code:

```{r, profanity_removal, echo=TRUE, eval=FALSE}
  #profanity removal
  
  if(profanity_clean==1){
    
    #download profane words and prep for matching
    data(profanity_zac_anger)
    data(profanity_alvarez)
    data(profanity_arr_bad)
    data(profanity_banned)
    data(profanity_racist)
    Sys.sleep(100)
    
    special_chars <- as_tibble(c("\\!", "\\@",  "\\#","\\$", "\\&","\\(","\\)",
                                 "\\-","\\‘","\\.","\\/","\\+",'\\"','\\“'))
    special_chars$replacement <- paste0("st",
                                        stri_rand_strings(nrow(special_chars),
                                        3, '[a-zA-Z0-9]'))
    
    profanity<-as_tibble(c(profanity_zac_anger,profanity_alvarez,
                           profanity_arr_bad,profanity_banned,
                           profanity_racist))
    profanity<-unique(profanity)
    profanity$replacement <- paste0("pr",
                                    stri_rand_strings(nrow(profanity), 
                                    7, '[a-zA-Z0-9]'))
    
    for(i in 1:nrow(special_chars)){
      profanity <- profanity %>% 
        mutate(value=gsub(special_chars[i,]$value, special_chars[i,]$replacement,
                          value))
    }
    profanity <- profanity%>% mutate(value=paste0('\\b', value, '\\b'))
    
    rm(profanity_zac_anger,profanity_alvarez,profanity_arr_bad,
       profanity_banned,profanity_racist)
    
    #clean up profane words, replace by random string
    
    for(i in 1:nrow(special_chars)){
      twitter_df <- twitter_df %>% 
        mutate(text=gsub(special_chars[i,]$value,
                         special_chars[i,]$replacement,
                         text))
    }
    
    for(i in 1:nrow(profanity)){
      twitter_df <- twitter_df %>% 
        mutate(text=gsub(profanity[i,]$value,
                         profanity[i,]$replacement,
                         text,ignore.case = TRUE))
    }
     
    
    twitter_df <- twitter_df %>% 
      mutate(text=gsub("st[a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9]",
                       "",text,ignore.case = TRUE))
 
    rm(profanity,special_chars)
  }
```

As a last step, the function *expand_urls* from the *longurl* package has been used to obtain the true domain names from the *t.co* URLs. This is done trough the below code:

```{r, get_domains, echo=TRUE, eval=FALSE}

split<-200
value <- round(nrow(processed_source)/split,0)
segments <- tibble(start=integer(),end=integer())
segments <- add_row(segments, start=1,end=value)

for(i in 2:(split-1)){
   segments <- add_row(segments, start=value*(i-1)+1,end=i*value)
}

   segments <- add_row(segments, start=value*(split-1)+1,
                       end=nrow(processed_source))

# first segment
   segment <- processed_source[segments[1,]$start:segments[1,]$end,]
   domains<-get_domains(segment,anonimised = 0)
   iteration <- 1

for(i in 179:split){
  segment <- processed_source[segments[i,]$start:segments[i,]$end,]
   domains_i<-get_domains(segment,anonimised = 0)
   domains <- rbind(domains,domains_i)
   iteration <-segments[i,]$start
}      

domains <-  domains %>% 
    mutate(domain = gsub(x=expanded_url, 
                         pattern = "(http|ftp|https)://",replacement = "")) %>%
    mutate(domain = gsub(x=domain, 
                         pattern = "www.", replacement = "")) %>%
    mutate(domain = gsub(x=domain, 
                         pattern = "ww[0-9].", replacement = "")) %>%
    mutate(domain = gsub(x = domain, 
                         pattern = "/S+", replacement = ""))

domains$domain <- domain(domains$expanded_url)
domains <-  domains %>% 
    mutate(domain = gsub(x=domain, pattern = "www.", replacement = "")) %>%
    mutate(domain = gsub(x=domain, pattern = "ww[0-9].", replacement = "")) %>%
    mutate(domain = gsub(x = domain, pattern = "m.", replacement = ""))

domains_list <- domains %>% select(domain) %>% unique(.)
domains_list$domain_key <- paste0("domain_",seq.int(nrow(domains_list)))
domains <- domains %>% left_join(domains_list,by="domain")
```

```{r, save_domains}
write_csv(domains,"domains.csv")
domains_sanitised <- domains %>% select(orig_url,domain=domain_key)
write_csv(domains_sanitised,"domains_clean.csv")
rm(domains_list)
```

As result of this processing, we have two data frames to use for further analysis: one with the processed tweets and another with a list of t.co URLs and domains. A sample of both is shown below:

```{r, show_processed_data, eval=TRUE}

processed_source %>% filter(id %in% c(1,6,33,49,160,914,3999,7336)) %>% 
  select(id,target,text,hashtag,link,mention) %>%
  kable(caption="Processed Data - Sample",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  column_spec(3,width = "15em") %>%
  kable_styling(latex_options =c("striped","hold_position","scale_down")) 
  
```


```{r, show_domains_data,eval=TRUE}
  domains_data %>% mutate(id=seq.int(nrow(domains_data))) %>% 
  filter(id %in% c(1,2,37,39,208,211,512,513)) %>%  select(-id) %>%
  kable(caption="t.co to Domain Translation - Sample",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","hold_position")) 
  
```

Both datasets in their "uncensored" and "sanitised" versions have been made available on [GitHub](https://github.com/carlosyanez/Tweets_Catastrophe_Classification).

Before proceeding, we will split this data into three datasets:

* a **training** dataset, to be used for further analysis, modelling and tuning.
* a **testing** dataset, to be used in tuning.
* a **validation** dataset, which will put aside and only use at the end to generate the last results.

Unless said otherwise all the below analysis has been done with the **training** dataset only!


```{r, download_processed files}

if(download_processed==TRUE){

  if(download_clean==TRUE){
    
    source_URL <- "https://raw.githubusercontent.com/carlosyanez/Tweets_Catastrophe_Classification/master/processed_source_clean.csv"
    domains_URL <- "https://raw.githubusercontent.com/carlosyanez/Tweets_Catastrophe_Classification/master/domains_clean.csv"
    source_file <- "processed_source_clean.csv"
    domains_file <- "domains_clean.csv"
  }
  else{
    source_URL <- "https://raw.githubusercontent.com/carlosyanez/Tweets_Catastrophe_Classification/master/processed_source.csv"
    domains_URL <- "https://raw.githubusercontent.com/carlosyanez/Tweets_Catastrophe_Classification/master/domains.csv"
    source_file <- "processed_source.csv"
    domains_file <- "domains.csv"
  }
  
  if(!file.exists(source_file)){
    download.file(source_URL,source_file)
  }
  if(!file.exists(domains_file)){
    download.file(domains_URL,domains_file)
  }
 
  source_data <- read_csv(source_file)
  domains_data <- read_csv(domains_file) %>% 
                select(link=orig_url,domain,domain_key)
   
  rm(source_URL,source_file,domains_URL,domains_file)
   
  version_number<-as.double(version$minor)/10+as.double(version$major)
  if(version_number<3.6){
      set.seed(200)
  }else{
    set.seed(200, sample.kind="Rounding")
  }

  validation_index <- createDataPartition(y = source_data$id, times = 1, p = 0.1, list = FALSE)
  training_test<- source_data[-validation_index,]
  validation <- source_data[validation_index,]

  test_index <- createDataPartition(y = training_test$id, times = 1, p = 0.1, list = FALSE)
  training0<- training_test[-test_index,]
  test0 <- training_test[test_index,]
  
  rm(source_file,domains_file,validation_index,training_test,test_index,version_number)


}

extra_stop_words <- tibble(word=c("t.co","http","https","û_","amp","û","1","2","3",
                                  "4","5","6","7","8","9","10","Ò","Ûª","ûªs","ûï","å","è",
                                  "u0089û_"))

```


## Scoring each tweet.

In order to use a machine learning method that allows us to determine whether any relationship between the content of each tweet and its target status, we need to be able to generate a function that rates each piece of content. A way of do this is to generate a numeric value for each tweet, that then can be used for estimate whether it is "catastrophic" or not.

In the case with text, a simple way to achieve this is to assign each constituent word a score and then use those to calculate a sentence value (with a simple way of calculating such value being adding to individual scores). The following lines explain the way this was done in this particular exercise - please note that as a starting point, we are making them assumption this is a good method - how to determine the right formula is a problem by itself.

Taking the each tweets processed text, the first step is to "tokenise" this dataset, i.e. split it in its component words - for this the function *unnest_tokens* from the *tidytext* package has been used. Since we would like to know how this words are split between catastrophic ("positive") and non-catastrophic ("negative") entries and perhaps use this a scoring base, we will also remove "stop words" (common words such as "and" "or", basic verbs,etc..), we will filter them using the *stop_words* dataset available on *tidytext*.

The code to achieve this goes as follows:

```{r unnest_sample, echo=TRUE, eval=FALSE}

  data("stop_words")
  extra_stop_words$lexicon <- "EXTRA"
  stop_words<-rbind(stop_words,extra_stop_words)
  rm(extra_stop_words)
  
 
  tokenised_words <- training_dataset %>% unnest_tokens(word,text)
  tokenised_words <- output$tokenised_words %>%
                            anti_join(stop_words, by="word") 
```

With this action, we effectively have created a dictionary of all word in the training set, mapped against the times where have beeen used in *catastrophic* (positives) and *non-catastrophic* (tweets). With this data, it would be terrific if we could build and indicator to represented how *catastrophic* each word is - which in turn can be use to calculate scores for new tweets. Ideally, this score should meet the following criteria:

* Properly weighs "disaster" related words.
* Penalises "non disaster" words
* Sits in the middle for more ambivalent terms.
* Creates a clear division, so when compounded into a tweet value it clearly helps to distinguish positives from negatives.

From the data at hand, it looks easy to start with a calculation of how frequent each word is, in general and in both positive and negative tweets. The chart below shows this, highlighting some selected words:

```{r word_numbers, eval=TRUE}
selected_words<- c("tongue","typhoon","suicide","fires","body","laughing","survive",
                   "shoulder","song","wrapped","tonight","property","disaster","hiroshima","cnn",
                   "wildfire","yellow","worse","volcano","response","weird","antebellum")

training_tokenised <-tokenise_data(training0,stop_words,domains_data,training_flag=TRUE) 

p<- training_tokenised$tokenised_words %>% group_by(word) %>%
  summarise(positives=sum(target==1), negatives=sum(target==0)) %>% ungroup() %>%
  ggplot(aes(negatives,positives)) +
  geom_point(data=(. %>% filter(!(word %in% selected_words))),
             color='grey',fill='grey',size=4,aes(text = paste("Word:", word))) +
  geom_point(data=(. %>% filter(word %in% selected_words)),
             color='navy',fill='navy',size=4,aes(text = paste("Word:", word))) +
  theme(legend.position="bottom") +
  labs(title="Pos. and neg. occurrences for each word (log scale)",y="Positives", x = "Negatives") +
  theme_grey() +
  geom_abline(color = "gray40", lty = 2)  +
  geom_text_repel(aes(label=ifelse((word %in% selected_words),as.character(word),'')),
        #    hjust=0,vjust=0,
        #    nudge_x = 0.02,
        #    nudge_y = 0.02,
            color="black",size=3) +
  scale_x_log10() +
  scale_y_log10()

p
rm(p)
```

From this chart, we can see that the angle (of the (negative,positive) vector) shows how words relate to positive tweets. This angle can be easily obtained with a cartesian to polar conversion. If we call the angle $\theta$, we can generate an initial score that looks like the below chart (chart against positive percentages for visualisation convenience).


```{r theta_chart, eval=TRUE}

scores1 <-calculate_scores(training_tokenised,word_parameters=c("sin",1,0,0,0))

p<- scores1$words %>%
  ggplot(aes(pos_proportion,theta_nn)) +
  geom_point(data=(. %>% filter(!(word %in% selected_words))),
             color='grey',fill='grey',size=4,aes(text = paste("Word:", word))) +
  geom_point(data=(. %>% filter(word %in% selected_words)),
             color='navy',fill='navy',size=4,aes(text = paste("Word:", word))) +
  theme(legend.position="bottom") +
  labs(title=" Positive Proportion vs theta",x="Positive Proportion", y = "theta") +
  theme_grey() +
  geom_abline(color = "gray40", lty = 2)  +
  geom_text_repel(aes(label=ifelse((word %in% selected_words),as.character(word),'')),
         #   hjust=0,vjust=0,
            nudge_x = 0.02,
            nudge_y = 0.003,
            color="black",size=3)

p
rm(p)
```


From this point, the formula has been refined to penalised non-disaster words. After several iterations, we have come up with the following formula:

$$pre\_score = sin(pos\_proportion)^{\lambda_1}*\frac{\theta}{90 \si{\degree} }$$

$$score = \frac{pre\_score}{max(pre\_score)} - \lambda_2$$

where

* $pos\_proportion$: Proportion of times where word was found in a "positive" tweet. This sinus of this parameter is used to help penalise less relevant words.
* $\lambda_1$: Parameterisation parameter, used to modulate the effect of less relevant words.
* $\theta$: angle of vector $(positive\_proportion,negative\_proportion)$, as discussed previously. 
* $\lambda_2$: Offset to potentially give a negative value to less relevant words, in order to provide compensation - e.g. in case a "disaster" word is used in figurative context amongst many non relevant words. This will be treated as a tuning parameter.

For illustration purposes, if we set $\lambda_1 = 1$ and $\lambda_2 = 0$, we obtain the following sample distribution:

```{r, scores_sample_0, eval=TRUE}
p<- scores1$words %>%
  ggplot(aes(pos_proportion,score)) +
  geom_point(data=(. %>% filter(!(word %in% selected_words))),
             color='grey',fill='grey',size=4,aes(text = paste("Word:", word))) +
  geom_point(data=(. %>% filter(word %in% selected_words)),
             color='navy',fill='navy',size=4,aes(text = paste("Word:", word))) +
  theme(legend.position="bottom") +
  labs(title=" Positive Proportion vs Score",x="Positive Proportion", y = "Score") +
  theme_grey() +
  geom_abline(color = "gray40", lty = 2)  +
  geom_text_repel(aes(label=ifelse((word %in% selected_words),as.character(word),'')),
     #       hjust=0,vjust=0,
            nudge_x = 0.02,
            nudge_y = -0.03,
            color="black",size=3)

p
rm(p,scores1)
```

The chart above shows we have achieved the objectives of giving distinct scores based on the  *catastrophe* value of each word. This however, does not reflect the amount of times a word appears - it may be the case that a particular term appears just one and gets an extreme score which is not really a good indicator of its real *disaster* value. Thus, a small modification of the formula is required:

$$score = 
 \begin{cases} 
      \emptyset & n \leq \lambda_3 \\
      \frac{pre\_score}{max(pre\_score)} - \lambda_2 & n > \lambda_3
   \end{cases}$$

where
* $n$ : number of times the word is appears in the training set,
* $\lambda_3$: arbitrary threshold to filter word with a low number of occurrences (and possibly biased). This is a tuning parameter.

Using $\lambda_3=10$ for illustration purposes, we have the below chart:

```{r, scores_sample_1, eval=TRUE}
scores1 <-calculate_scores(training_tokenised,word_parameters=c("sin",1,0,0,0))
scores2 <-calculate_scores(training_tokenised,word_parameters=c("sin",1,0,10,0))

scores1 <- scores1$word %>% anti_join(scores2$word, by="word") %>% mutate(score=0)

p<- scores2$words %>%
  ggplot(aes(pos_proportion,score)) +
  geom_point(data=(. %>% filter(!(word %in% selected_words))),
             color='grey',fill='grey',size=4,aes(text = paste("Word:", word))) +
  geom_point(data=(. %>% filter(word %in% selected_words)),
             color='navy',fill='navy',size=4,aes(text = paste("Word:", word))) +
   geom_point(data=(scores1 %>% filter(!(word %in% selected_words))),
             color='grey',fill='grey',size=4,aes(text = paste("Word:", word))) +
    geom_point(data=(scores1 %>% filter((word %in% selected_words))),
             color='navy',fill='navy',size=4,aes(text = paste("Word:", word))) +
  theme(legend.position="bottom") +
  labs(title=" Positive Proportion vs Score - n>10",x="Positive Proportion", y = "Score") +
  theme_grey() +
  geom_abline(color = "gray40", lty = 2)  +
  geom_text_repel(aes(label=ifelse((word %in% selected_words)&(pos_proportion>0.2),as.character(word),'')),
            direction="x",
            force=30,
          #  hjust=0,vjust=0,
           nudge_x = 0.002,
          #  nudge_y = -0.003,
            color="black",size=3) +
  geom_text_repel(aes(label=ifelse((word %in% (selected_words[which(selected_words=="song")]))&(pos_proportion<=0.2),as.character(word),'')),
            direction="y",
            force=50,
          #  hjust=0,vjust=0,
            nudge_x = 0.002,
            nudge_y = 0.0003,
            color="black",size=3) +
    geom_text_repel(data=scores1,aes(label=ifelse((word %in% selected_words),as.character(word),'')),
            direction="y",
            force=4,
           # hjust=0,vjust=0,
            nudge_x = 0.02,
          #  nudge_y = 0.003,
            color="black",size=3)

p

rm(p,scores1,scores2)
```


Then, we would like to know if these word scores produce a set of tweet scores that are a potential good input for a regression function. If we using a sum to calculate a score per tweet, this can be implemented in R with the below code:

```{r calculate_score, echo=TRUE, eval=FALSE}

   tweet_score <- tokenised_words %>% 
                 left_join(word_scores,by="word") %>% filter(!is.na(score)) %>%
                 group_by(id) %>% summarise(word_score=sum(score)) %>% 
                 ungroup()
```


which, will produce results like the below graphs (for different offset values):


```{r tweet_scores, eval=TRUE}
offset<-0
scores2 <-calculate_scores(training_tokenised,word_parameters=c("sin",1,0,10,offset))
scores3<-score_tweets(training0,training_tokenised,scores2) 
p1<- scores3$vector %>% ggplot(aes(id,word_score,color=target)) +geom_point() + theme(legend.position="bottom") +
  labs(title=paste("Tweet Score - offset: ",offset),x="id", y = "Score") +
  theme_grey()
offset<-0.2
scores2 <-calculate_scores(training_tokenised,word_parameters=c("sin",1,0,10,offset))
scores3<-score_tweets(training0,training_tokenised,scores2) 
p2<- scores3$vector %>% ggplot(aes(id,word_score,color=target)) +geom_point() + theme(legend.position="bottom") +
  labs(title=paste("Tweet Score - offset: ",offset),x="id", y = "Score") +
  theme_grey()
offset<-0.4
scores2 <-calculate_scores(training_tokenised,word_parameters=c("sin",1,0,10,offset))
scores3<-score_tweets(training0,training_tokenised,scores2) 
p3<- scores3$vector %>% ggplot(aes(id,word_score,color=target)) +geom_point() + theme(legend.position="bottom") +
  labs(title=paste("Tweet Score - offset: ",offset),x="id", y = "Score") +
  theme_grey()
offset<-0.6
scores2 <-calculate_scores(training_tokenised,word_parameters=c("sin",1,0,10,offset))
scores3<-score_tweets(training0,training_tokenised,scores2) 
p4<- scores3$vector %>% ggplot(aes(id,word_score,color=target)) +geom_point() + theme(legend.position="bottom") +
  labs(title=paste("Tweet Score - offset: ",offset),x="id", y = "Score") +
  theme_grey()

grid.arrange(p1, p2,p3,p4, nrow=2)

rm(score2,score3,offset,p1,p2,p3,p4)
```

We can see a diffuse division between positive and negative tweets. Of course, this could potentially be improved with better modelling, however we will consider it acceptable as a first step and for the purposes of this report. 

A similar process has been done to create tweet scores for hashtags, links(or rather the domains of the *unshortened* URLs) and mentions. After a bit of "trial and error", a slightly different formula has been chosen for these three attributes, which is shown below:

$$pre\_score= \left( \frac{pos\_proportion}{\lambda_1} \right)^{\lambda_{2}} \cdot \frac{\theta}{90}$$
$$ score =
            \begin{cases}
            \emptyset & n \leq \lambda_4 \\
            \frac{pre\_score}{max(pre\_score)}  - \lambda_3 & n > \lambda_4 \\
            \end{cases}$$

where 

* $pos\_proportion$ is the proportion of the term in the entire corpus of positive hashtags/domains/mentions,
* $\lambda_1$ tuning parameter that re-scales - everything above the threshold is relevant, the rest not so much,
* $\lambda_2$ tuning parameter to weigh down irrelevant terms and weigh up relevant ones,
* $\theta$ angle of vector $(negatives,positives)$,
* $\lambda_3$, Offset to potentially give a negative value to less relevant term, in order to provide compensation - e.g. in case a "disaster" word is used in figurative context amongst many non relevant term This will be treated as a tuning parameter.
* $\lambda_4$, threshold of number of occurrences necessary to consider the score as valid.


The below charts show an example of score generated using this method:


```{r other_scores, eval=TRUE}

scores2 <-calculate_scores(training_tokenised,
                           word_parameters=c("sin",1,0,10,0),
                           hashtag_parameters=c("sin",4,0.5,0),
                           handle_parameters=c("sin",4,0.5,0),
                           link_parameters=c("sin",4,0.5,0))

scores3<-score_tweets(training0,training_tokenised,scores2) 

p1<- scores3$vector %>% filter(!is.na(hashtag_score)) %>%
  ggplot(aes(id,hashtag_score,color=target)) +geom_point() + theme(legend.position="bottom") +
  labs(title=paste("Hashtag Score"),x="id", y = "Score") +
  theme_grey()
p2<- scores3$vector %>% filter(!is.na(link_score)) %>%
  ggplot(aes(id,hashtag_score,color=target)) +geom_point() + theme(legend.position="bottom") +
  labs(title=paste("Link Score"),x="id", y = "Score") +
  theme_grey()
p3<- scores3$vector %>% filter(!is.na(handle_score)) %>%
  ggplot(aes(id,hashtag_score,color=target)) +geom_point() + theme(legend.position="bottom") +
  labs(title=paste("Mention/Handle Score"),x="id", y = "Score") +
  theme_grey()

grid.arrange(p1,p2,p3,ncol=2)
rm(p1,p2,p3,scores2,scores3)
```


In order to facilitate the next steps, the above process has been written into three functions:

* **tokenise_data**, which takes the data in the pre-processed format and generates data frames with the token words, hashtags, domains and handles.
* **calculate_scores**, which takes the output of the previous function (*tokenised* data) and calculate the scores for each word, hashtag, link and handle. All the previously discussed tuning parameter have been defined as inputs for this function.
* **score_tweets**, which will generate a score for each tweet, for each of the aforementioned categories.

All these functions are written in the accompanying .R file, with in-line commentary explaining their key  steps.

## Finalising the model and data quality.

From here, we can move to the next step and to generate the last part of the model - the one that will render a prediction.

Considering the work done so far, there are potentially many ways to use to the scores to create a model for prediction. After weighing the options, the following two-step approach will used:

1. First, a regression algorithm will be used to train a model for each attribute - i.e. 4 different and independent predictions will be generated. The result of this process will be vector for each tweet, with the results of each prediction (1,0 or potentially N/A if there is no such attribute for a given tweet).
2. Then, this vector will be used as an input for a second machine learning method , which will attempt to predict the target result based on the individual responses.

For this process, two functions have been written in this report .R file:

* **fit_model**, which is the training function. Its inputs are the scores plus relevant tuning parameters. It will output 5  machine learning models (for each attribute plus the aggregate one).
* **predict_values**, which will take the training score, the test data and the models previously generated. it will output the predictions and measure accuracy if the target value is provided.

These functions leverage the caret package, and they have been written to provide some flexibility: for example **fit_model** allows to be run using different machine learning models. For further details, please refer to the detailed commentary in the .R file.

For the initial run - that will help us establish an initial benchmark - the general linear model (glm) in caret is used for the individual predictions. For the aggregate prediction, we will use [xgbBoost](https://en.wikipedia.org/wiki/XGBoost). The below code is used:


```{r first_prediction}
training_tokenised <- tokenise_data(training0,extra_stop_words,domains_data,training_flag=TRUE)
scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,10,0),
                             hashtag_parameters=c("exp",4,0.5,3),
                             handle_parameters=c("exp",4,0.5,4),
                             link_parameters=c("exp",4,0.75,4))
training_vector <- score_tweets(training0,training_tokenised,scores) 
fitting_model <- fit_model(training_vector)
results_1<-predict_values(test0,fitting_model,scores,extra_stop_words,domains_data)
```

The confusion matrix for this example is:

```{r fp_acc}
results_1$eval %>% select(Method=eval,Accuracy=result_agg) %>%
  kable(caption="Initial results",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","HOLD_position")) 


```


```{r confusion_matrix11, eval=TRUE}

if(eval==TRUE){
  results_comparison <- tibble(attempt=numeric(),accuracy=numeric())
  results_comparison <- add_row(results_comparison,
                              attempt="Initial training",
                              accuracy=results_1$eval[which(results_1$eval$eval=="aggregate"),]$result_method)
}

results_1_cm <- confusionMatrix(results_1$results$target,results_1$results$aggregate)
fourfoldplot(results_1_cm$table,
             color = c("#B22222", "#2E8B57"),
               main = "") #+ 
    #text(-0.4,0.4, "TN", cex=1) + 
    #text(0.4, -0.4, "TP", cex=1) + 
    #text(0.4,0.4, "FN", cex=1) + 
    #text(-0.4, -0.4, "FP", cex=1) 
rm(results_1_cm)
```

As observed, there is a significant amount of false positives and false negatives. A sample  is provided in the below tables.

```{r result_1_FP,eval=TRUE}
orig_FP<-c(269,3581,1026,9779,10380,10402)
results_1$results_extended %>% filter(category == "FP")  %>%
  select(id,target,original_text)  %>%
  filter(id %in% orig_FP) %>%
  kable(caption="Initial results - False Positives",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  column_spec(3,width = "30em") %>%
  kable_styling(latex_options =c("striped","HOLD_position","scale_down")) 

```

```{r result_1_FN,eval=TRUE}
orig_FN <- c(3,253,1172,1968,6571)
results_1$results_extended %>% filter(category == "FN")  %>%
  select(id,target,original_text)  %>%
  filter(id %in% orig_FN) %>%
  kable(caption="Initial results - False Negatives",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  column_spec(3,width = "30em") %>%
  kable_styling(latex_options =c("striped","HOLD_position","scale_down")) 

```

A more detailed review of both sets, shows the source file classification seems to contain errors: tweets that are seemingly not a catastrophe are tagged as such and vice-versa. One particular example of this are tweets allusive to the Hiroshima nuclear bombing. Those tweets are intended as  remembrance/memorial and hence are not related to a current catastrophe. However they are classified as "disaster".

```{r hiroshima_tweets,eval=TRUE}
results_1$results_extended %>% filter(grepl("Hiroshima",text)) %>% select(id,target,original_text)  %>%
  kable(caption="Initial results - Hiroshima Tweets",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  column_spec(3,width = "30em") %>%
  kable_styling(latex_options =c("striped","HOLD_position")) 
```

Although a complication, this is a good example of the complexity of processing natural language:

1. Quality training data is essential. Bad data will probably result in bad results - even with a very sophisticated algorithm.

2. Human language is complicated. Meaning of each word is highly contextual and changes depending on the register, dialect and over time. This can be difficult for a machine learning model to *grasp*.

As a way to mitigate this, false positives and false negatives have been manually reviewed and their new positive/negative result has been modified accordingly. After feeding them back into the training dataset (and selecting a new testing sample, and running the machine learning model), the below results are obtained:

```{r corrected_results0}
corrections <- "target_corrected.csv"
manual_scores <- "manual_scores.csv"

if(!file.exists(corrections)){
  download.file("https://raw.githubusercontent.com/carlosyanez/Tweets_Catastrophe_Classification/master/target_corrected.csv",corrections)
  }

if(!file.exists(manual_scores)){
  download.file("https://raw.githubusercontent.com/carlosyanez/Tweets_Catastrophe_Classification/master/manual_scores.csv",manual_scores)
}

corrected_data <- read_csv(corrections)
manual_scores <-  read_csv(manual_scores)

initial_set <- rbind(training0,test0)
q1 <- 0.1*round(nrow(initial_set),0)

corrected_results <- initial_set %>% left_join(corrected_data,by="id") %>% 
  mutate(target=ifelse(is.na(target_corrected),target,target_corrected)) %>% 
  filter(!is.na(target_corrected)) %>%
  select(id,target,text,keyword,location,hashtag,link,mention,original_text)

mixing_set <- initial_set %>% anti_join(corrected_data,by="id")
q2 <- nrow(mixing_set)
percent_value <- q1/q2
  
version_number<-as.double(version$minor)/10+as.double(version$major)
if(version_number<3.6){
  set.seed(200)
}else{
    set.seed(200, sample.kind="Rounding")
}

test_index <- createDataPartition(y = mixing_set$id, times = 1, p = percent_value, list = FALSE)
training<- mixing_set[-test_index,]
test <- mixing_set[test_index,]

training <- rbind(training,corrected_results)
rm(q1,q2,initial_set,mixing_set,percent_value,corrected_results)
```

```{r corrected_results1}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)
scores <- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,10,0),
                             hashtag_parameters=c("exp",4,0.5,3),
                             handle_parameters=c("exp",4,0.5,4),
                             link_parameters=c("exp",4,0.75,4))
training_vector <- score_tweets(training,training_tokenised,scores) 
fitting_model <- fit_model(training_vector)
results_2<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)

rm(training_old,version_number)
```

```{r fp_acc0}
results_2$eval %>% select(Component=eval,Accuracy=result_agg) %>%
  kable(caption="Initial results - corrected dataset",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","hold_position")) 

if(eval==TRUE){
                results_comparison <- add_row(results_comparison,
                              attempt="Initial training - Data Correction",
                              accuracy=results_2$eval[which(results_3$eval$eval=="aggregate"),]$result_method)
              }
```


```{r results_comparison_1_2,eval=TRUE}
results_comparison[1:2,] %>%
  kable(caption="After Data Correction",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","hold_position")) 
```



This illustrates that better training data can lead to better results. It is also worth noticing this may well reflect a real-life scenario, where a social media monitoring centre is constantly reviewing false positives and false negatives with the purpose of continously re-training a model, to adapt to changes language, emerging events, etc.

Of course the other attributes can also benefit of this *curated* approach. In this report, and for illustration purposes - the  list of hashtags in the training set has been reviewed. There are some  hashtags that most probably mean catastrophe (e.g. like #earthquake) and other than most probably don't (like #ashes2017, which is cricket related).

Using a sample set of curated hashtags, it is possible to observe a small increment in accuracy (shown in the below table). Handles and links will be left as a action to improve for the time being.

```{r}
scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,10,0),
                             hashtag_parameters=c("exp",4,0.5,3),
                             handle_parameters=c("exp",4,0.5,4),
                             link_parameters=c("exp",4,0.75,4),
                            manual_scores = manual_scores)
training_vector <- score_tweets(training,training_tokenised,scores) 
fitting_model <- fit_model(training_vector)
results_3<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
```


```{r fp_acc3}
results_3$eval %>% select(Component=eval,Accuracy=result_agg) %>%
  kable(caption="Initial results - Curated Hashtags",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","hold_position")) 

if(eval==TRUE){
results_comparison <- add_row(results_comparison,
                              attempt="Curated hashtags",
                              accuracy=results_3$eval[which(results_3$eval$eval=="aggregate"),]$result_method)
}
```


```{r results_comparison_1_0,eval=TRUE}
results_comparison[1:3,] %>%
  kable(caption="After Hashtag Curation",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","hold_position")) 
```


## Tuning

With this new and *improved* dataset, the next step is to tune the model. Looking at how it's constructed, there are *many* aspects to tune, namely:

* The parameters used to calculate the tweet, hashtag, link and mention scores.
* The selection of classification algorithms used.
* The tuning parameters of each classification algorithm.

Tuning every single parameter will result in quite a lengthy exercise. Instead, this report will focus on some of the key components. Then  it will discus what works and what else could be done.

The first aspect to tune is the scoring function for word in training set. This is perhaps the key component of the model. From the previous section, we have that this score is calculated through the below formulas:

$$pre\_score = sin(pos\_proportion)^{\lambda_1}*\frac{\theta}{90 \si{\degree} }$$
$$score = 
 \begin{cases} 
      \emptyset & n \leq \lambda_3 \\
      \frac{pre\_score}{max(pre\_score)} - \lambda_2 & n > \lambda_3
   \end{cases}$$
  

where

* $pos\_proportion$: Proportion of times where a word was found in a "positive" tweet. This sinus of this parameter is used to help penalise less relevant words.
* $\lambda_1$: Parameterisation parameter, used to mitigate the effect of less relevant words.
* $\theta$: angle of vector $(positive\_proportion,negative\_proportion)$, as discussed previously. 
* $\lambda_2$: Offset to potentially give a negative value to less relevant words, in order to provide compensation - e.g. in case a "disaster" word is used in figurative context amongst many non relevant words. This will be treated as a tuning parameter.
* $n$ : number of times the word is appears in the training set,
* $\lambda_3$: arbitrary threshold to filter word with a low number of occurrences. This is a tuning parameter.

In this report, $\lambda_2$ and $\lambda_3$ will be tuned, which the intent of seeing how the penalisation of irrelevant words and minimum number of occurrences affect the model. 

The results of  $\lambda_2$'s tuning are shown below.

```{r word_lambda_2_tuning}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)

range <- seq(0,0.5,0.05)
stats <- tibble(parameter=numeric(),accuracy=numeric(),word_accuracy=numeric())
for (i in range){
                    scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,10,i),
                             hashtag_parameters=c("exp",4,0.5,3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
                    training_vector <- score_tweets(training,training_tokenised,scores) 
                    fitting_model <- fit_model(training_vector)
                    results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
                    acc <- results$eval[which(results$eval$eval=="aggregate"),]$result_method
                    w_acc <- results$eval[which(results$eval$eval=="word"),]$result_method
                    stats <- add_row(stats,parameter=i,accuracy=acc,word_accuracy=w_acc)
                  }

stats_1 <-stats
rm(range,acc,w_acc,stats)
```


```{r  word_lambda_2_tuning_results,eval=TRUE}
stats_1 %>% ggplot(aes(parameter,accuracy)) +
  geom_point(color='grey',fill='grey',size=4) +
  geom_point(data=(. %>% filter(accuracy == stats_1[which.max(accuracy),]$accuracy)),
             color='red',fill='red',size=4) +
  theme(legend.position="bottom") +
  labs(title="Word Score - lambda_2 tuning",x="lambda_2", y = "Accuracy") +
  theme_grey() 

word_lambda_2 <- stats_1[which.max(stats_1$accuracy),]$parameter
if(eval==TRUE){
results_comparison <- add_row(results_comparison,
                              attempt="Optimised word score - lambda_2",
                              accuracy=stats_1[which.max(stats_1$accuracy),]$accuracy)
}

```

As shown above, the best result is obtained with $\lambda_2$=`r word_lambda_2`. It is also worth noticing that increasing this value won't necessarily result in better results, since it will then penalise "neutral" terms too.

With this value, we will then also find if there is an optimal $\lambda_3$, i.e. to see if removing the words that rarely appear have an negative impact in the model. The results are shown below:


```{r word_lambda_3_tuning}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)

range <- seq(0,30,1)
stats <- tibble(parameter=numeric(),accuracy=numeric(),word_accuracy=numeric())
for (i in range){
                    scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,i,word_lambda_2),
                             hashtag_parameters=c("exp",4,0.5,3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
                    training_vector <- score_tweets(training,training_tokenised,scores) 
                    fitting_model <- fit_model(training_vector)
                    results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
                    acc <- results$eval[which(results$eval$eval=="aggregate"),]$result_method
                    w_acc <- results$eval[which(results$eval$eval=="word"),]$result_method
                    stats <- add_row(stats,parameter=i,accuracy=acc,word_accuracy=w_acc)
                  }

stats_2<-stats
rm(range,acc,w_acc,stats)
```


```{r word_lambda_3_tuning_results,eval=TRUE}
word_lambda_3 <- 10
stats_2 %>% ggplot(aes(parameter,accuracy)) +
  geom_point(color='grey',fill='grey',size=4) +
  geom_point(data=(. %>% filter(accuracy == stats_2[which.max(accuracy),]$accuracy)),color='red',fill='red',size=4) +
  geom_point(data=(. %>% filter(parameter == word_lambda_3)),color='orange',fill='orange',size=4) +
  theme(legend.position="bottom") +
  labs(title="Word Score - lambda_3 tuning",x="lambda_3", y = "Accuracy") +
  theme_grey() 

#word_lambda_3 <- stats_2[which.max(stats_2$accuracy),]$parameter

if(eval==TRUE){
results_comparison <- add_row(results_comparison,
                              attempt="Optimised word score - lambda_3",
                              accuracy=stats_2[which(stats_2$parameter==word_lambda_3),]$accuracy)
}
```


This result is a bit more interesting. As observed above, the first optimal $\lambda_3$ is quite a low value, however there is a relative bigger decrease in accuracy around its neighbours when compared against the second best $\lambda_3$. It is important to notice that the accuracy  depends on the corpus of words in the test set and how they are combined into sentences. This is of course random and it will change if the training data changes. Even though we can do cross validation, perhaps it is not a bad a idea to choose a perhaps sub-optimal value for the sake of consistency - at least until more thorough tuning is possible. Therefore, to continue with this exercise, a value of $\lambda_3$=`r word_lambda_3` has been chosen.

After the word scoring, perhaps the most relevant component in the model is the scoring function for hashtags (given that hashtag intend to be **meaningful** descriptors). Similarly to the word scoring, we will tune the equivalent parameters hashtag's $\lambda_2$ and $\lambda_3$.

The results for the tuning of $\lambda_2$ are shown below:


```{r hashtag_lambda_2_tuning}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)

range <- seq(0.1,2,0.2)
stats <- tibble(parameter=numeric(),accuracy=numeric(),hashtag_accuracy=numeric())
for (i in range){
                    scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,word_lambda_3,word_lambda_2),
                             hashtag_parameters=c("exp",4,i,3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
                    training_vector <- score_tweets(training,training_tokenised,scores) 
                    fitting_model <- fit_model(training_vector)
                    results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
                    acc <- results$eval[which(results$eval$eval=="aggregate"),]$result_method
                    h_acc <- results$eval[which(results$eval$eval=="hashtag"),]$result_method
                    stats <- add_row(stats,parameter=i,accuracy=acc,hashtag_accuracy=h_acc)
                  }

stats_3<-stats
rm(range,acc,w_acc,stats)
```

```{r hashtag_lambda_2_tuning_results,eval=TRUE}
p1 <- stats_3 %>% ggplot(aes(parameter,accuracy)) +
  geom_point(color='grey',fill='grey',size=4) +
  geom_point(data=(. %>% filter(accuracy == stats_3[which.max(accuracy),]$accuracy)),color='red',fill='red',size=4) +
  theme(legend.position="bottom") +
  labs(title="Hashtag Score - lambda_2 tuning - General Acc. ",x="lambda_2", y = "Accuracy") +
  theme_grey() 

p2 <- stats_3 %>% ggplot(aes(parameter,accuracy)) +
  geom_point(color='grey',fill='grey',size=4) +
  geom_point(data=(. %>% filter(accuracy == stats_3[which.max(accuracy),]$accuracy)),color='red',fill='red',size=4) +
  theme(legend.position="bottom") +
  labs(title="Hashtag Score - lambda_2 tuning - Hashtag Acc.",x="lambda_2", y = " Hashtag Accuracy") +
  theme_grey() 

grid.arrange(p1,p2,nrow=2)
rm(p1,p2)
hashtag_lambda_2 <- 0.5
```

The first graph shows the overall accuracy results (comparable to the word tuning), while the second graph shows the results where there is a hashtag. From them, the following points can be made:

* Smaller variations in the overall score can be explained by the fact that not all tweets have hashtags, thus the effect of tuning this part of the model will obviously have a limited effect on the general result.
* Expanding the above point, it looks like its contribution to general score it is mostly when hashtags do indicate a catastrophe (and therefore have a bigger score). It is worth noticing that many of the high-score hashtag were not ranked by the algorithm but manually added - which is perhaps and indication that manual curation is not necessarily a bad idea - especially when data is poor quality or insufficient.

With this mind, the tuning of $\lambda_3$ will help to determine the overall effect of hashtag occurrence. The results are shown in the below chart.

```{r hashtag_lambda_3}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)

range <- seq(2,24,1)
stats <- tibble(parameter=numeric(),accuracy=numeric(),hashtag_accuracy=numeric())
for (i in range){
                    scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,word_lambda_3,word_lambda_2),
                             hashtag_parameters=c("exp",4,hashtag_lambda_2,i),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
                    training_vector <- score_tweets(training,training_tokenised,scores) 
                    fitting_model <- fit_model(training_vector)
                    results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
                    acc <- results$eval[which(results$eval$eval=="aggregate"),]$result_method
                    h_acc <- results$eval[which(results$eval$eval=="hashtag"),]$result_method
                    stats <- add_row(stats,parameter=i,accuracy=acc,hashtag_accuracy=h_acc)
                  }

stats_4<-stats
rm(range,acc,w_acc,stats)
```

```{r hashtag_lambda_3_tuning_results,eval=TRUE}
stats_4%>% ggplot(aes(parameter,accuracy)) +
  geom_point(color='grey',fill='grey',size=4) +
  geom_point(data=(. %>% filter(accuracy == stats_4[which.max(accuracy),]$accuracy)),color='red',fill='red',size=4) +
  theme(legend.position="bottom") +
  labs(title=" Hashtag Score - lambda_3 tuning - General Acc.",x="lamdba_3", y = "Accuracy") +
  theme_grey() 

hashtag_lambda_3 <- 20
if(eval==TRUE){
results_comparison <- add_row(results_comparison,
                              attempt="Optimised hashtag score - lambda_3",
                              accuracy=stats_4[which(stats_4$parameter==hashtag_lambda_3),]$accuracy)
}
```

This result seems similar in nature to the tuning for quantity cut-off for the word scoring.

Looking at the results so far (table below), this tuning process has only resulted in small gains when compared with the initial data corrections. Considering that the effect of hashtags is small, in this report the tuning of the links and handle scoring functions will not be conducted. This doesn't mean they may bring an improved accuracy, but perhaps are not worth the effort with the existing training dataset.

```{r results_comparison_2,eval=TRUE}
results_comparison[1:6,] %>%
  kable(caption="Initial Comparison",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","HOLD_position")) 
```

```{r}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)
scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,word_lambda_3,word_lambda_2),
                             hashtag_parameters=c("exp",4,hashtag_lambda_2,hashtag_lambda_3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
training_vector <- score_tweets(training,training_tokenised,scores) 
 
                 
#word_grid <-  expand.grid(nIter = seq(2, 20, 1))


fitting_model <- fit_model(training_vector,method_word="glm",# tuneGrid_word = word_grid,
                      method_hashtag="glm",
                      method_link="glm",
                      method_handle="glm",
                      method_aggregate="xgbTree")

results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
word_model_1_0<- fitting_model$word        
stats_5_1 <- results$eval
scores$hashtags
```

Instead, the decision is to focus on the tuning of the classification functions. At this point, the model uses the caret package to leverage to known machine learning algorithms, namely:

* *glm* (as in generalised linear model) in its logistic regression mode to turn word, hashtag, link and mention to make a prediction on those components independently.
* *xgbTree* (extreme gradient boosting) to predict an "overall" positive or negative based on the previous predictions.

Considering that the assignment instructions state

> *For this project, you will be applying machine learning techniques that go beyond standard linear regression*

the next stage will be to asses if there is any model that produces better results than glm. (Although so far this is not a **standard**  linear regression model any more.)

After investigating a number of possible algorithms, the [**LogitBoost**](https://en.wikipedia.org/wiki/LogitBoost) will use as comparison. This algorithm is available in R through the [**caTools**](https://www.rdocumentation.org/packages/caTools/versions/1.17.1/topics/LogitBoost) package.

As a starting point, we will try this algorithm for the word score classification function. The below results show the best result after tuning its only parameter, which is the number of iterations.

```{r word_logitboost}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)
scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,word_lambda_3,word_lambda_2),
                             hashtag_parameters=c("exp",4,hashtag_lambda_2,hashtag_lambda_3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
training_vector <- score_tweets(training,training_tokenised,scores) 

word_grid <-  expand.grid(nIter = seq(2, 20, 1))

fitting_model <- fit_model(training_vector,method_word="LogitBoost", tuneGrid_word = word_grid,
                      method_hashtag="glm",
                      method_link="glm",
                      method_handle="glm",
                      method_aggregate="xgbTree")

results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
word_model_1<- fitting_model$word        
stats_5 <- results$eval
```


```{r word_logitboost_result, eval=TRUE}
word_model_1$results %>% select(Iterations=nIter,Accuracy) %>% 
  ggplot(aes(Iterations,Accuracy)) +
  geom_point(color='red',fill='red',size=4) +
  theme(legend.position="bottom") +
  labs(title="Word Score - LogitBoost tuning",x="Iterations", y = "Accuracy") +
  theme_grey() 

if(eval==TRUE){  
results_comparison <- add_row(results_comparison,
                              attempt="Word score - LogitBoost",
                              accuracy=stats_5[which(stats_5$eval=="aggregate"),]$result_method)
}
```


Please note graph above shows the accuracy results for the LogitBoost regression only, not for the entire model. However, it shows that in this case the tuning has not effect. 
In this case, the overall accuracy equals *`r stats_5[which(stats_5$eval=="aggregate"),]$result_method`*, which is no better than the one obtained with a simple logistic regression. Similar results are obtained when the same comparison is repeated for the other scoring components.


```{r other_logitboost}
training_tokenised <- tokenise_data(training,extra_stop_words,domains_data,training_flag=TRUE)
scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,word_lambda_3,word_lambda_2),
                             hashtag_parameters=c("exp",4,hashtag_lambda_2,hashtag_lambda_3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
training_vector <- score_tweets(training,training_tokenised,scores) 
 
tuning_grid <-  expand.grid(nIter = seq(2, 20, 1))

fitting_model <- fit_model(training_vector,method_word="glm",
                      method_hashtag="LogitBoost", tuneGrid_hashtag = tuning_grid,
                      method_link="LogitBoost" , tuneGrid_link = tuning_grid,
                      method_handle="LogitBoost", tuneGrid_handle = tuning_grid,
                      method_aggregate="xgbTree")

results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)
hashtag_model_2 <- fitting_model$hashtag
link_model_2 <- fitting_model$link
handle_model_2 <- fitting_model$handle
aggregate_model_2 <- fitting_model$aggregate
stats_6 <- results$eval
```


```{r other_logitboost_result0,eval=TRUE}

p1<- hashtag_model_2$results %>% select(Iterations=nIter,Accuracy) %>% 
  ggplot(aes(Iterations,Accuracy)) +
  geom_point(color='red',fill='red',size=4) +
  theme(legend.position="bottom") +
  labs(title="Hashtag Score - LogitBoost tuning",x="Iterations", y = "Accuracy") +
  theme_grey() 

p2<- link_model_2$results %>% select(Iterations=nIter,Accuracy) %>% 
  ggplot(aes(Iterations,Accuracy)) +
  geom_point(color='red',fill='red',size=4) +

  theme(legend.position="bottom") +
  labs(title="Link Score - LogitBoost tuning",x="Iterations", y = "Accuracy") +
  theme_grey() 

p3<- handle_model_2$results %>% select(Iterations=nIter,Accuracy) %>% 
  ggplot(aes(Iterations,Accuracy)) +
  geom_point(color='red',fill='red',size=4) +
  theme(legend.position="bottom") +
  labs(title="Handle Score - LogitBoost tuning",x="Iterations", y = "Accuracy") +
  theme_grey()

p1
p2
p3

if(eval==TRUE){
results_comparison <- add_row(results_comparison,
                              attempt="Other scores - LogitBoost",
                              accuracy=stats_6[which(stats_6$eval=="aggregate"),]$result_method)
}
rm(p1,p2,p3)
```

In this case, the accuracy value equals  **`r stats_5[which(stats_5$eval=="aggregate"),]$result_method`**, which is worse  than the logistic regression result.  

These results shouldn't be confusing - a more "sophisticated" method does not necessarily mean a better result  the important thing is to find the model that better represents the problem at hand (when possible). Simpler models may also have an advantage in terms of processing time - which may be a key factor to consider even if other models perform marginally better. 


Completed the above steps, there is one aspect left to tune : the XGBoost algorithm. This method has several parameters, for which a good explanation of them can be found [here](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/). 

Given the number of parameters, a full sweep could take very, very long. However, the caret packages already does some tuning by default. We will use the results from the previous iteration as a starting point.


```{r xgbTree_best_tune, eval=TRUE}
aggregate_model_2$bestTune %>% kable(caption="XGBoost - caret's Best Tune",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","HOLD_position")) 
```

Below is the code used for tuning:

```{r xgbTree_tuning, echo=TRUE, eval=FALSE}
training_tokenised <- tokenise_data(training,extra_stop_words,
                                    domains_data,training_flag=TRUE)
scores<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,
                                               word_lambda_3,word_lambda_2),
                             hashtag_parameters=c("exp",4,                                                                       hashtag_lambda_2,
                                                  hashtag_lambda_3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
training_vector <- score_tweets(training,training_tokenised,scores) 
 
aggregate_tuning_grid <- expand.grid(nrounds = seq(45,55,1),  
                                     max_depth = 1:3,
                                     eta = seq(0.2,0.4,0.1), 
                                     gamma =seq(0,1,0.1),
                                     colsample_bytree = seq(0.7,0.9,0.1), 
                                     min_child_weight = 1:3, 
                                     subsample = 0.75) 

fitting_model <- fit_model(training_vector,method_word="glm",
                      method_hashtag="glm", 
                      method_link="glm", 
                      method_handle="glm", 
                      method_aggregate="xgbTree",
                      tuneGrid_aggregate = aggregate_tuning_grid)

results<-predict_values(test,fitting_model,scores,extra_stop_words,domains_data)

aggregate_model_3 <- fitting_model$aggregate
stats_7 <- results$eval
```

The optimised solution results in an accuracy of **`r stats_7[which(stats_7$eval=="aggregate"),]$result_method`**. The optimised XGBoost parameters are:

```{r xgbTree_best_tune2, eval=TRUE}
aggregate_model_3$bestTune %>% kable(caption="XGBoost - Best Tune",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","HOLD_position")) 

if(eval==TRUE){
results_comparison <- add_row(results_comparison,
                              attempt="Optimised XGBoost",
                              accuracy=stats_7[which(stats_7$eval=="aggregate"),]$result_method)
}
```


When compared with all the previous results, we have the below table:

```{r results_after_training, eval=TRUE}
results_comparison %>%
  kable(caption="Initial Comparison",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","HOLD_position")) 
```

As shown above, the greatest gains in accuracy are due to the data correction and the tuning of the word scoring formula. Further tuning only produces worse or marginally better results (from hashtag optimisation to optimised XGBoost, for example). Furthermore, it seems that there is a ceiling that has been reached, perhaps due to the limitation of the model or quality of the training data (bad inputs -> bad results).

Nevertheless, up to this point we have touched on all the relevant aspects of the model. Therefore (for the purpose of the this assignment), the only step left is to assess against the validation data and then comment based on those results.

\newpage
# Results
<!-- A results section that presents the modelling results and discusses the model performance. -->

As the last step of this report, the model has been assessed against the **validation** dataset that was set aside a the beginning of this exercise. In order to do this, the below steps have been followed:

1. **Training** and **test** set have been merged into one dataset.
2. The model has been trained against this new dataset.
3. The newly-trained model has been used to generate predictions for the **validation** dataset

```{r}

training_validation <- rbind(training,test)

training_tokenised_validation <- tokenise_data(training_validation, extra_stop_words,domains_data,training_flag=TRUE)
scores_validation<- calculate_scores(training_tokenised,
                             word_parameters=c("sin",1,0,word_lambda_3,word_lambda_2),
                             hashtag_parameters=c("exp",4,hashtag_lambda_2,hashtag_lambda_3),
                             handle_parameters=c("exp",4,0.8,4),
                             link_parameters=c("exp",4,0.9,4),
                              manual_scores = manual_scores)
training_vector_validation <- score_tweets(training_validation,training_tokenised_validation,scores_validation) 
 

aggregate_tuning_grid <- expand.grid(nrounds = 45, #53 
                                     max_depth = 1, #4
                                     eta = 0.2,  #0.3
                                     gamma =0.2, #0
                                     colsample_bytree = 0.8, #0.7
                                     min_child_weight = 3, #7
                                     subsample = 0.75) #0.7

fitting_model_validation <- fit_model(training_vector_validation,method_word="glm",
                      method_hashtag="glm",
                      method_link="glm", 
                      method_handle="glm", 
                      method_aggregate="xgbTree", tuneGrid_aggregate = aggregate_tuning_grid)
```

```{r}
results_validation<-predict_values(validation,fitting_model,scores,extra_stop_words,domains_data)
```



```{r validation_results}
#The result of this is displayed in the below accuracy summary

results_validation$eval  %>% select(Component=eval,Accuracy=result_agg) %>%
  kable(caption="XGBoost - Best Tune",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  kable_styling(latex_options =c("striped","HOLD_position")) 
```

This results in an accuracy value of *`r results_validation$eval[which(results_validation$eval$eval=="aggregate"),]$result_method`**. The confusion matrix summary is shown below:

```{r validation_cm,eval=TRUE}

validation_cm<-confusionMatrix(as.factor(results_validation$results_extended$target),
                as.factor(results_validation$results_extended$aggregate))

fourfoldplot(validation_cm$table,
            color = c("#B22222", "#2E8B57"),
           main = "") #+ 
           #text(-0.4,0.4, "TN", cex=1) + 
           #text(0.4, -0.4, "TP", cex=1) + 
           #text(0.4,0.4, "FN", cex=1) + 
           #text(-0.4, -0.4, "FP", cex=1)
```

Looking at this result, it looks like we have reached some sort of ceiling for the model. As mentioned before, possible culprits are the model or the data. Considering that at the start of the this report data issues were flagged, it wouldn't be unreasonable to suspect that is a main contributor. To confirm this, both false positives and false negatives were inspected - below are a sample of both groups:

```{r validation_FP, eval=TRUE}
results_validation$results_extended  %>% filter(category=="FP") %>%
  select(id,target,original_text) %>%
 filter(id %in% c(490,650,2153,10402,9600,10492,5434))%>% 
  kable(caption="Validation Results - False Positives",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  column_spec(3,width = "30em") %>%
  kable_styling(latex_options =c("striped","HOLD_position","scale_down"))
```


```{r validation_FN, eval=TRUE}
results_validation$results_extended %>%  filter(category=="FN") %>%
  select(id,target,original_text) %>%
 filter(id %in% c(283,391,1172,1772,2341,3259,4310,9781,9795)) %>% 
  kable(caption="Validation Results - False Negatives",format = "latex", booktabs = TRUE,
        format.args = list(decimal.mark = '.',   big.mark = " ")) %>% 
  column_spec(3,width = "30em") %>%
  kable_styling(latex_options =c("striped","HOLD_position","scale_down"))

```

After inspecting for set, it looks like there is a problem of misclassification with the false negatives. In this case the model *correctly* predicts most of them as not a disaster tweet. However, upon inspection it seems the model is clearly *incorrectly* classifying most of all false positives. It is worth noticing though that all of them contain *catastrophic* words. A possible reason of this that the model is mostly driven by the words rather than the other attributes and its limited lexicon results in wrong scores for those tweets. Fortunately, it is possible to see for those tweets whether a prediction for each component was generated of not, which is shown in the below charts, for false positives and all validation tweets.

```{r validation_predictions_fp, eval=TRUE}
agg_summary <- results_validation$results_extended %>%
  select(category,aggregate) %>% 
  gather(method,prediction,-category)%>% filter(!is.na(prediction)) %>%
  group_by(category,method) %>% 
  summarise(positive=sum(prediction=="1"),negative=sum(prediction=="0")) %>% ungroup()

word_summary <- results_validation$results_extended %>%
  select(category,word=word_pred) %>% 
  gather(method,prediction,-category)%>% filter(!is.na(prediction)) %>%
  group_by(category,method) %>% 
  summarise(positive=sum(prediction=="1"),negative=sum(prediction=="0")) %>% ungroup()

hashtag_summary <- results_validation$results_extended %>%
  select(category,hashtag=hashtag_pred) %>% 
  gather(method,prediction,-category)%>% filter(!is.na(prediction)) %>%
  group_by(category,method) %>% 
  summarise(positive=sum(prediction=="1"),negative=sum(prediction=="0")) %>% ungroup()

handle_summary <- results_validation$results_extended %>%
  select(category,handle=handle_pred) %>%
  gather(method,prediction,-category)%>% filter(!is.na(prediction)) %>%
  group_by(category,method) %>% 
  summarise(positive=sum(prediction=="1"),negative=sum(prediction=="0")) %>% ungroup()

link_summary <- results_validation$results_extended %>%
  select(category,link=link_pred) %>%
  gather(method,prediction,-category) %>%filter(!is.na(prediction)) %>%
  group_by(category,method) %>% 
  summarise(positive=sum(prediction=="1"),negative=sum(prediction=="0")) %>% ungroup()

overall_summary <- rbind(agg_summary,word_summary)
overall_summary <- rbind(overall_summary,hashtag_summary)
overall_summary <- rbind(overall_summary,handle_summary)
overall_summary <- rbind(overall_summary,link_summary)
rm(agg_summary,word_summary,hashtag_summary,handle_summary,link_summary)

overall_summary <- overall_summary %>% gather(prediction,value,-category,-method)

overall_summary %>% filter(category=="FP") %>%
  filter(!is.na(value)) %>% filter(!(value==0)) %>%
  ggplot(aes(x=method,y=value,fill=prediction,label=value)) + geom_col() + 
  theme(legend.position="bottom") +
  labs(title="Count of Predictions per component - False Positives",x="Component Prediction", y = "Count") +
  theme_grey() + geom_text(position = position_stack(vjust = 0.5))
```

```{r overall_summary, eval=TRUE}

overall_summary %>% select(-category) %>% 
    filter(!is.na(value)) %>% filter(!(value==0)) %>%
  group_by(method,prediction) %>% summarise(value=sum(value)) %>% ungroup() %>%
  ggplot(aes(x=method,y=value,fill=prediction,label=value)) + geom_col() +
  labs(title="Count of Predictions per component -Val. Results",x="Component Prediction", y = "Count") + geom_text(position = position_stack(vjust = 0.5)) +
  theme_grey() 
  
```

Both results show - interestingly that the overall results is almost entirely driven by the word scoring model. In fact, this model is basically ignoring the other attributes. This can be explained by the attribute distribution in the training data. As shown in below graph, the tweets that only have words exceed by far any other combination, which probably results in the XGBoost algorithm ignoring all other components. Although in theory hashtags, links and handles make sense, the existing data is making them irrelevant.

```{r validation_training, eval=TRUE}
training_vector_validation$split %>% select(type,n_perc) %>% mutate(percentage=round(n_perc,2)) %>%
  ggplot(aes(x=type,y=percentage,label=percentage)) %>%
  + geom_col(fill="#add8e6") +
  labs(title="Training set split by attributes",x="Component Prediction", y = "Percentage (%)") +
  coord_flip() +
  geom_text(position = position_stack(vjust = 0.5)) +
  theme_grey() 
```

This probably only compounds another problem previously mentioned: the absence of several words in the lexicon created by the training data. This is summarised by the below chart. 

```{r tokenise_validation, eval=TRUE}

validation_tokenised <- tokenise_data(validation, extra_stop_words,domains_data)

absent <- unique(validation_tokenised$tokenised_words %>% select(word)) %>%
left_join((training_tokenised_validation$tokenised_words %>% select(word,target)),by="word") %>% filter(is.na(target)) %>% mutate(target="absent") %>% unique(.) %>% filter(!is.na(word))

present <- unique(validation_tokenised$tokenised_words %>% select(word)) %>%
left_join((training_tokenised_validation$tokenised_words %>% select(word,target)),by="word") %>% filter(!is.na(target)) %>% mutate(target="present") %>% unique(.) %>% filter(!is.na(word))

rbind(absent,present) %>% group_by(target) %>%
  summarise(q=n()) %>% ungroup %>%
  ggplot(aes(x=target,y=q,label=q)) +
   geom_col(fill="#add8e6") +
  labs(title="Validation Words - Presence in Training Lexicon",x="Presence", y = "Count") +
  coord_flip() +
  geom_text(position = position_stack(vjust = 0.5))+
  theme_grey() 

 rm(absent,present)
```


The consequence of this will be incorrect or incomplete scoring - this is one probable reason for the high number of false negatives. Again, the conclusion seems to be that good quality training data, in reasonable quantities is critical for a good machine learning model.

\newpage
# Conclusion
<!-- A conclusion section that gives a brief summary of the report, its potential impact, its limitations, and future work. -->

In this report, we have attempted to create a machine learning model that correctly predicts whether a tweet correspond to a *disaster* (or *catastrophic*) event. A model to achieve this has been proposed, which combines individual predictions for each of the key available attributes of each tweet into a combined score, using the *XGBoost* algorithm. Through, successive data corrections and tuning the model the results were incrementally improved. Tested against the validation set, an accuracy of **`r results_validation$eval[which(results_validation$eval$eval=="aggregate"),]$result_method`** was achieved. 

Clearly, this is far from perfect. However, through its flaws, it was possible to draw a number of valuable conclusions which reflect relevant challenges applicable both to this particular case and to general practice. These main take aways are:

* A good quality training dataset is essential. Poorly curated training data will probably result in substandard or incorrect predictions (the discipline is called **Data** Science after all!)
* Like in problem in Science/Engineering, coming up with a model that properly fits the problem is also essential. There aren't universal solutions and more *sophisticated* and popular models are not silver bullets. Sometimes, simpler linear models will be the best available fit - considering both accuracy, computing resources and the intention behind solving the problem.
* Tuning can be an enormous undertaking. Again, it is important to understand the purpose of behind the problem and be practical.
* In relation to this problem, natural language can be especially challenging. A simple model like this cannot really interpret meaning the same way a human does. The dictionary is always growing and changing - even one word can mean different things.

If someone would like to continue this work, there are multiple paths to keep exploring, namely:

* First, seek for a better dataset and run again to see the effects. Better dataset not only means the right classification but also:
** a greater proportion of hashtags, links and handles to see how they can better supplement the words in the tweet.
** a more diverse vocabulary to have a better collection of disaster and non-disaster words.
* In addition, a dataset with other property would be able also help to improve this model, for instance:
** rate tweets based on user ids (to differentiate between MMORPG players and defence forces, identify shock jocks, emergency institutions, earthquake watchers,etc.).
** add a time and location function, which can be cross-referenced with other sources for catastrophic events (e.g. give a higher rating to all tweets from a particular city when a natural disaster strikes)
* Also, it would be interesting to see how this model rates against other natural language classification problems, like the [Onion or Not dataset in Kaggle](https://www.kaggle.com/chrisfilo/onion-or-not/kernels).
 
 
Last but definitely not least, this model needs better a mathematical justification behind the scoring functions. Those functions were defined to fit the expected results, but a better statistical understanding is required. This may be a big task but perhaps a necessary one to produce a "production" quality model.

<!-- EOF -->
