\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[11pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Arial}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Data Science Capstone - Tweet Classsification System - Catastrophe or Not?},
            pdfauthor={Carlos Yáñez Santibáñez},
            colorlinks=true,
            linkcolor=blue,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in,headheight=70pt,headsep=0.3in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{amsmath}
\pagestyle{fancy}
\fancyhead[L]{PH125.9x}
\fancyhead[R]{Data Science : Capstone}
\fancyfoot[L]{C. Yáñez Santibáñez}
\fancyfoot[C]{ }
\fancyfoot[R]{\thepage}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\title{Data Science Capstone - Tweet Classsification System - Catastrophe or
Not?}
\author{Carlos Yáñez Santibáñez}
\date{April 10, 2020}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\newpage

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

As mentioned in the Introduction, this report presents the attempt to
build a machine learning model to classify Tweets in a binary model. In
this particular case, the question to answer is whether those tweets
correspond to a catastrophic event or not. The idea for this was taken
of a Kaggle Competition for Starters
(\href{\%5Bhttps://www.kaggle.com/c/nlp-getting-started\%5D}{Real or
Not? NLP with Disaster Tweets}). Instead of using the data provided in
Kaggle, in this report the original version in
\href{https://www.figure-eight.com/data-for-everyone/}{Data For
Everyone} is used.

To start, let's have look at sample data:

\begin{table}[!h]

\caption{\label{tab:show_source_data}Sample Data}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{rr>{\raggedright\arraybackslash}p{15em}ll}
\toprule
id & target & text & keyword & location\\
\midrule
\rowcolor{gray!6}  1 & 1 & Just happened a terrible car crash & NA & NA\\
6 & 1 & All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected & NA & NA\\
\rowcolor{gray!6}  33 & 0 & London is cool ;) & NA & NA\\
49 & 1 & @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C & ablaze & Birmingham\\
\rowcolor{gray!6}  160 & 0 & 'The harder the conflict the more glorious the triumph.' Thomas Paine & aftershock & 304\\
\addlinespace
914 & 1 & Uganda Seen as a Front Line in the Bioterrorism Fight & bioterrorism & Alaska\\
\rowcolor{gray!6}  3 999 & 0 & I forgot to bring chocolate with me. Major disaster. & disaster & Los Angeles, London, Kent\\
7 336 & 1 & Finnish ministers: Fennovoima nuclear reactor will go ahead http://t.co/vB3VFm76ke \#worldnews \#news \#breakingnews & nuclear\%20reactor & World\\
\bottomrule
\end{tabular}}
\end{table}

As seen above, the dataset cointains the following observations (to
match Kaggle's format):

\begin{itemize}
\tightlist
\item
  \textbf{id} : uniqe sequential identifier.
\item
  \textbf{target} : indicator of whether the tweet corresponds to a
  catastrophic event (\textbf{1}) or not (\textbf{0}).
\item
  \textbf{text} : tweet, as extracted from Twitter.
\item
  \textbf{keyword} : search term used in Twitter's search bar to
  retrieve the tweets.
\item
  \textbf{location} : Name of the place wherre the tweet in question
  originated.
\end{itemize}

For this report, \textbf{keyword} and \textbf{location} will be ignored,
choosing to focus on the text (tweet) only. After a brief observation of
its content, in this analysis the text will be divided into four
component:

\begin{itemize}
\tightlist
\item
  The text proper. i.e.~the \emph{natural language} sentence in the
  text. In the code, this will be called \textbf{words}. All the tweets
  in this file are written in English language, but they may contain
  non-English characters, emojis and emoticons.
\item
  The \textbf{hashtags}, which are in user created categorisation. This
  is a meaningful component of the tweet and may help clarify whether
  the message is a real disaster. For example, in below tweet, the
  hashtag helps us to ellucidate this is cricket commentary rather than
  a fire disaster:
\end{itemize}

\begin{table}[H]

\caption{\label{tab:tweet_hasthag_example}Tweet about cricket}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{r>{\raggedright\arraybackslash}p{40em}}
\toprule
id & text\\
\midrule
1 674 & Australia's Ashes disaster - how the collapse unfolded at Trent Bridge| cricket\\
\bottomrule
\end{tabular}}
\end{table}

\begin{itemize}
\tightlist
\item
  The \textbf{links} contained in the tweet - perhaps the respective
  Internet domain could help distinguish ``quality'' disaster sources
  (e.g.~official announcement from relevant authority, The Red
  Cross/Crescent) from non-disistarr ones (e.g.~gossip magazine using
  hyperbolic language). To obtain meaningful content though, the links
  need to be ``unshortened'' from the t.co addresses provided by
  Twitter.
\item
  Who is mentioned in the tweet (by their Twitter \textbf{handles}).
  Similarly to the previous points, it may be useful to distinguish
  between people reaching out to government/emergency service from
  \emph{\href{https://en.wikipedia.org/wiki/Shock_jock}{shock jocks}}
  using florid language.
\end{itemize}

\hypertarget{processing-and-cleaning-up-the-data}{%
\subsection{Processing and cleaning up the
data}\label{processing-and-cleaning-up-the-data}}

In order to obtain the data in the desired format for analysis, the
function \emph{prepare\_data} has been created. Its code performs the
below activities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extract hasthags, links and Twitter handles from the tweet.
\item
  Clean up the remaining text, removing links and handles, and
  performing transformation operations on emojis,emoticons and other
  special characters.
\item
  (Optional) Replace swear words and profane language with unique
  strings, in case seeing those wants to be avoided, yet retaining the
  words for analysis. Please note this report has been generated with
  the uncensored version, thus results may vary.
\item
  ``Unshorten'' link URLs to obtain domain names of linked sources.
\end{enumerate}

The first step is similar for the three components - below is the code
use to remove the hashtags:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ source_data}\OperatorTok{$}\NormalTok{hashtag <-}\StringTok{ }\KeywordTok{str_extract_all}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text, }\StringTok{"#}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{S+"}\NormalTok{)}
\NormalTok{  source_data <-}\StringTok{ }\NormalTok{source_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{hashtag =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{hashtag, }
           \DataTypeTok{pattern =} \StringTok{"character}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{(0)"}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{hashtag =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{hashtag, }\DataTypeTok{pattern =} \StringTok{"c}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{hashtag =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{hashtag, }\DataTypeTok{pattern =} \StringTok{"}\CharTok{\textbackslash{}"}\StringTok{"}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{hashtag =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{hashtag, }\DataTypeTok{pattern =} \StringTok{")"}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{hashtag =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{hashtag, }\DataTypeTok{pattern =} \StringTok{","}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

In the following step, both links and mentions are removed, keeping the
hashtags since they may also be part of the sentence in question. Then,
leveraging the functions in the
\textbf{\href{https://www.rdocumentation.org/packages/textclean/versions/0.9.3}{textclean}}
package, the remaining data is further normalised, by replacing :

\begin{itemize}
\tightlist
\item
  contractions with full words,
\item
  emojis and emoticos with equivalent word (e.g. :) with `smile'),
\item
  all non-ASCII characters wit equivalent,
\item
  internet slang with full words,
\item
  html code,
\item
  money symbols,
\item
  numbers with full words,
\item
  ordinals with full words replace\_ordinal,
\item
  timestamps.
\end{itemize}

The code that achieves this is shown below:

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{#remove mentions and links from text, save original text in different observation}
  
\NormalTok{   source_data}\OperatorTok{$}\NormalTok{original_text <-}\StringTok{ }\NormalTok{source_data}\OperatorTok{$}\NormalTok{text}
\NormalTok{   source_data <-}\StringTok{ }\NormalTok{source_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{( }\DataTypeTok{text =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ text, }\DataTypeTok{pattern =} \StringTok{"#"}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{))  }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{( }\DataTypeTok{text =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ text, }\DataTypeTok{pattern =} \StringTok{"@}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{S+"}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{text =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{text, }
                       \DataTypeTok{pattern =} \StringTok{"(s?)(f|ht)tp(s?)://}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{S+}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, }
                       \DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }
  
\CommentTok{#further clean text with textclean package}
  
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_contraction}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\StringTok{ }\KeywordTok{replace_emoji}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\StringTok{ }\KeywordTok{replace_emoticon}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_non_ascii}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text,}\DataTypeTok{impart.meaning=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_internet_slang}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_html}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_money}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_number}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_ordinal}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\NormalTok{  source_data}\OperatorTok{$}\NormalTok{text<-}\KeywordTok{replace_time}\NormalTok{(source_data}\OperatorTok{$}\NormalTok{text)}
\end{Highlighting}
\end{Shaded}

An optional step is to remove swear and profane vocabulary from the
text, in case that is desired. This has been done through the below
code:

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{#profanity removal}
  
  \ControlFlowTok{if}\NormalTok{(profanity_clean}\OperatorTok{==}\DecValTok{1}\NormalTok{)\{}
    
    \CommentTok{#download profane words and prep for matching}
    \KeywordTok{data}\NormalTok{(profanity_zac_anger)}
    \KeywordTok{data}\NormalTok{(profanity_alvarez)}
    \KeywordTok{data}\NormalTok{(profanity_arr_bad)}
    \KeywordTok{data}\NormalTok{(profanity_banned)}
    \KeywordTok{data}\NormalTok{(profanity_racist)}
    \KeywordTok{Sys.sleep}\NormalTok{(}\DecValTok{100}\NormalTok{)}
    
\NormalTok{    special_chars <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{!"}\NormalTok{, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{@"}\NormalTok{,  }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{#"}\NormalTok{,}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{$"}\NormalTok{, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{&"}\NormalTok{,}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{,}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{,}
                                 \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{-"}\NormalTok{,}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{‘"}\NormalTok{,}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{,}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{/"}\NormalTok{,}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{+"}\NormalTok{,}\StringTok{'}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{"'}\NormalTok{,}\StringTok{'}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{“'}\NormalTok{))}
\NormalTok{    special_chars}\OperatorTok{$}\NormalTok{replacement <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"st"}\NormalTok{,}
                                        \KeywordTok{stri_rand_strings}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(special_chars),}
                                        \DecValTok{3}\NormalTok{, }\StringTok{'[a-zA-Z0-9]'}\NormalTok{))}
    
\NormalTok{    profanity<-}\KeywordTok{as_tibble}\NormalTok{(}\KeywordTok{c}\NormalTok{(profanity_zac_anger,profanity_alvarez,}
\NormalTok{                           profanity_arr_bad,profanity_banned,}
\NormalTok{                           profanity_racist))}
\NormalTok{    profanity<-}\KeywordTok{unique}\NormalTok{(profanity)}
\NormalTok{    profanity}\OperatorTok{$}\NormalTok{replacement <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"pr"}\NormalTok{,}
                                    \KeywordTok{stri_rand_strings}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(profanity), }
                                    \DecValTok{7}\NormalTok{, }\StringTok{'[a-zA-Z0-9]'}\NormalTok{))}
    
    \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(special_chars))\{}
\NormalTok{      profanity <-}\StringTok{ }\NormalTok{profanity }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{value=}\KeywordTok{gsub}\NormalTok{(special_chars[i,]}\OperatorTok{$}\NormalTok{value, special_chars[i,]}\OperatorTok{$}\NormalTok{replacement,}
\NormalTok{                          value))}
\NormalTok{    \}}
\NormalTok{    profanity <-}\StringTok{ }\NormalTok{profanity}\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{value=}\KeywordTok{paste0}\NormalTok{(}\StringTok{'}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{b'}\NormalTok{, value, }\StringTok{'}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{b'}\NormalTok{))}
    
    \KeywordTok{rm}\NormalTok{(profanity_zac_anger,profanity_alvarez,profanity_arr_bad,}
\NormalTok{       profanity_banned,profanity_racist)}
    
    \CommentTok{#clean up profane words, replace by random string}
    
    \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(special_chars))\{}
\NormalTok{      twitter_df <-}\StringTok{ }\NormalTok{twitter_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{text=}\KeywordTok{gsub}\NormalTok{(special_chars[i,]}\OperatorTok{$}\NormalTok{value,}
\NormalTok{                         special_chars[i,]}\OperatorTok{$}\NormalTok{replacement,}
\NormalTok{                         text))}
\NormalTok{    \}}
    
    \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(profanity))\{}
\NormalTok{      twitter_df <-}\StringTok{ }\NormalTok{twitter_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{text=}\KeywordTok{gsub}\NormalTok{(profanity[i,]}\OperatorTok{$}\NormalTok{value,}
\NormalTok{                         profanity[i,]}\OperatorTok{$}\NormalTok{replacement,}
\NormalTok{                         text,}\DataTypeTok{ignore.case =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{    \}}
     
    
\NormalTok{    twitter_df <-}\StringTok{ }\NormalTok{twitter_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{      }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{text=}\KeywordTok{gsub}\NormalTok{(}\StringTok{"st[a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9]"}\NormalTok{,}
                       \StringTok{""}\NormalTok{,text,}\DataTypeTok{ignore.case =} \OtherTok{TRUE}\NormalTok{))}
 
    \KeywordTok{rm}\NormalTok{(profanity,special_chars)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

As a last step, the function \emph{expand\_urls} from the \emph{longurl}
package has been leveraged to unshorten the t.co URLs, to obtain the
domains of the links. This is done trough the below code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{split<-}\DecValTok{200}
\NormalTok{value <-}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(processed_source)}\OperatorTok{/}\NormalTok{split,}\DecValTok{0}\NormalTok{)}
\NormalTok{segments <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{start=}\KeywordTok{integer}\NormalTok{(),}\DataTypeTok{end=}\KeywordTok{integer}\NormalTok{())}
\NormalTok{segments <-}\StringTok{ }\KeywordTok{add_row}\NormalTok{(segments, }\DataTypeTok{start=}\DecValTok{1}\NormalTok{,}\DataTypeTok{end=}\NormalTok{value)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{(split}\DecValTok{-1}\NormalTok{))\{}
\NormalTok{   segments <-}\StringTok{ }\KeywordTok{add_row}\NormalTok{(segments, }\DataTypeTok{start=}\NormalTok{value}\OperatorTok{*}\NormalTok{(i}\DecValTok{-1}\NormalTok{)}\OperatorTok{+}\DecValTok{1}\NormalTok{,}\DataTypeTok{end=}\NormalTok{i}\OperatorTok{*}\NormalTok{value)}
\NormalTok{\}}

\NormalTok{   segments <-}\StringTok{ }\KeywordTok{add_row}\NormalTok{(segments, }\DataTypeTok{start=}\NormalTok{value}\OperatorTok{*}\NormalTok{(split}\DecValTok{-1}\NormalTok{)}\OperatorTok{+}\DecValTok{1}\NormalTok{,}
                       \DataTypeTok{end=}\KeywordTok{nrow}\NormalTok{(processed_source))}

\CommentTok{# first segment}
\NormalTok{   segment <-}\StringTok{ }\NormalTok{processed_source[segments[}\DecValTok{1}\NormalTok{,]}\OperatorTok{$}\NormalTok{start}\OperatorTok{:}\NormalTok{segments[}\DecValTok{1}\NormalTok{,]}\OperatorTok{$}\NormalTok{end,]}
\NormalTok{   domains<-}\KeywordTok{get_domains}\NormalTok{(segment,}\DataTypeTok{anonimised =} \DecValTok{0}\NormalTok{)}
\NormalTok{   iteration <-}\StringTok{ }\DecValTok{1}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{179}\OperatorTok{:}\NormalTok{split)\{}
\NormalTok{  segment <-}\StringTok{ }\NormalTok{processed_source[segments[i,]}\OperatorTok{$}\NormalTok{start}\OperatorTok{:}\NormalTok{segments[i,]}\OperatorTok{$}\NormalTok{end,]}
\NormalTok{   domains_i<-}\KeywordTok{get_domains}\NormalTok{(segment,}\DataTypeTok{anonimised =} \DecValTok{0}\NormalTok{)}
\NormalTok{   domains <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(domains,domains_i)}
\NormalTok{   iteration <-segments[i,]}\OperatorTok{$}\NormalTok{start}
\NormalTok{\}      }

\NormalTok{domains <-}\StringTok{  }\NormalTok{domains }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{domain =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{expanded_url, }
                         \DataTypeTok{pattern =} \StringTok{"(http|ftp|https)://"}\NormalTok{,}\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{domain =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{domain, }
                         \DataTypeTok{pattern =} \StringTok{"www."}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{domain =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{domain, }
                         \DataTypeTok{pattern =} \StringTok{"ww[0-9]."}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{domain =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ domain, }
                         \DataTypeTok{pattern =} \StringTok{"/S+"}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{))}

\NormalTok{domains}\OperatorTok{$}\NormalTok{domain <-}\StringTok{ }\KeywordTok{domain}\NormalTok{(domains}\OperatorTok{$}\NormalTok{expanded_url)}
\NormalTok{domains <-}\StringTok{  }\NormalTok{domains }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{domain =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{domain, }\DataTypeTok{pattern =} \StringTok{"www."}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{domain =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x=}\NormalTok{domain, }\DataTypeTok{pattern =} \StringTok{"ww[0-9]."}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{domain =} \KeywordTok{gsub}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ domain, }\DataTypeTok{pattern =} \StringTok{"m."}\NormalTok{, }\DataTypeTok{replacement =} \StringTok{""}\NormalTok{))}

\NormalTok{domains_list <-}\StringTok{ }\NormalTok{domains }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(domain) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{(.)}
\NormalTok{domains_list}\OperatorTok{$}\NormalTok{domain_key <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"domain_"}\NormalTok{,}\KeywordTok{seq.int}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(domains_list)))}
\NormalTok{domains <-}\StringTok{ }\NormalTok{domains }\OperatorTok{%>%}\StringTok{ }\KeywordTok{left_join}\NormalTok{(domains_list,}\DataTypeTok{by=}\StringTok{"domain"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As result of this processing, we have two data frames to use for further
analysis: one with the processed tweets and another with a list of t.co
URLs and domains. A sample of boths is shown below:

\begin{table}[!h]

\caption{\label{tab:show_processed_data}Sample Data}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{rr>{\raggedright\arraybackslash}p{15em}lll}
\toprule
id & target & text & hashtag & link & mention\\
\midrule
\rowcolor{gray!6}  1 & 1 & Just happened a terrible car crash & NA & NA & NA\\
6 & 1 & All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are excuse me tongue sticking out ected & NA & NA & NA\\
\rowcolor{gray!6}  33 & 0 & London is cool wink & NA & NA & NA\\
49 & 1 & Wholesale Markets ablaze & NA & http://t.co/lHYXEOHY6C & @bbcmtd\\
\rowcolor{gray!6}  160 & 0 & 'The harder the conflict the more glorious the triumph.' Thomas Paine & NA & NA & NA\\
\addlinespace
914 & 1 & Uganda Seen as a Front Line in the Bioterrorism Fight & NA & NA & NA\\
\rowcolor{gray!6}  3 999 & 0 & I forgot to bring chocolate with me. Major disaster. & NA & NA & NA\\
7 336 & 1 & Finnish ministers: Fennovoima nuclear reactor will go ahead worldnews news breakingnews & \#worldnews \#news \#breakingnews & http://t.co/vB3VFm76ke & NA\\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[!h]

\caption{\label{tab:show_domains_data}Sample Data}
\centering
\begin{tabular}[t]{lll}
\toprule
link & domain & domain\_key\\
\midrule
\rowcolor{gray!6}  https://t.co/irWqCEZWEU & bbc.co.uk & domain\_1\\
https://t.co/lHYXEOHY6C & twitter.com & domain\_2\\
\rowcolor{gray!6}  https://t.co/pmlOhZuRWR & sigalert.com & domain\_17\\
https://t.co/GKYe6gjTk5 & lawsociety.org.uk & domain\_18\\
\rowcolor{gray!6}  https://t.co/FaXDzI90dY & change.org & domain\_61\\
\addlinespace
https://t.co/SB5R7ShcCJ & change.org & domain\_61\\
\rowcolor{gray!6}  https://t.co/ZNTg2wndmJ & twitter.com & domain\_2\\
https://t.co/JLRw0Oi9ee & twitter.com & domain\_2\\
\bottomrule
\end{tabular}
\end{table}

Both datasets in their ``uncensored'' and ``sanitised'' versions have
been made available on
\href{https://github.com/carlosyanez/Tweets_Catastrophe_Classification}{GitHub}.

Before proceeding, we will split this data into three datasets:

\begin{itemize}
\tightlist
\item
  a \textbf{training} dataset, to be used for further analysis,
  modelling and tuning.
\item
  a \textbf{testing} dataset, to be used in tuning.
\item
  a \textbf{validation} dataset, which will put aside and only use at
  the end to generate the last results.
\end{itemize}

Unless said otherwise all the below analysis has been done with the
\textbf{training} dataset only!

\hypertarget{scoring-each-tweet.}{%
\subsection{Scoring each tweet.}\label{scoring-each-tweet.}}

In order to use a machine learning method that allows us to determine
whether any relationship between the content of each tweet and its
target status, we need to be able to generate a function that rates each
piece of content. A way of do this is to generate a numeric value for
each tweet, that then can be used for estimate whether it is
``catastrophic'' or not.

In the case with text, a simple way to achieve this is to assign each
constituent word a score and then use those to calculate a sentence
value (with a simple way of calculating such value being adding to
individual scores). The following lines explain the way this was done in
this particular exercise - please note that as a starting point, we are
making them assumption this is a good method - how to determine the
right formula is a problem by itself.

Taking the each tweets processed text, the first step is to ``tokenise''
this dataset, i.e.~split it in its component words - for this the
function \emph{unnest\_tokens} from the \emph{tidytext} package has been
used. Since we would like to know how this words are split between
catasthropic (``positive'') and non-catastrophic (``negative'') entries
and perhaps use this a scoring base, we will also remove ``stop words''
(common words such as ``and'' ``or'', basic verbs,etc), we will filter
them using the \emph{stop\_words} dataset available on \emph{tidytext}.

The code to achieve this goes as follows:

\begin{Shaded}
\begin{Highlighting}[]
  \KeywordTok{data}\NormalTok{(}\StringTok{"stop_words"}\NormalTok{)}
\NormalTok{  extra_stop_words}\OperatorTok{$}\NormalTok{lexicon <-}\StringTok{ "EXTRA"}
\NormalTok{  stop_words<-}\KeywordTok{rbind}\NormalTok{(stop_words,extra_stop_words)}
  \KeywordTok{rm}\NormalTok{(extra_stop_words)}
  
 
\NormalTok{  tokenised_words <-}\StringTok{ }\NormalTok{training_dataset }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unnest_tokens}\NormalTok{(word,text)}
\NormalTok{  tokenised_words <-}\StringTok{ }\NormalTok{output}\OperatorTok{$}\NormalTok{tokenised_words }\OperatorTok{%>%}
\StringTok{                            }\KeywordTok{anti_join}\NormalTok{(stop_words, }\DataTypeTok{by=}\StringTok{"word"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

With the data, we can calculate how frequent each word is, in general
and in both positive and negative tweets. The chart below shows this,
highlighting some selected words for analysis:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/word_numbers-1} \end{center}

Since we are looking at the impact of the word for a tweet being
positive, the above chart can be reduced to one indicator, the
proportion each words appears positive tweets against its total number
of appearances.

From here, the question is how to take this into a score for each word
that:

\begin{itemize}
\tightlist
\item
  Properly weighs ``disaster'' related words.
\item
  Penalises ``non disaster'' words but without reducing the impact of
  more ambivalent terms.
\item
  Allows to establish a clear division, so when compoundend it clearly
  helps to distinguish positives from negatives.
\end{itemize}

After thinking how to achieve this objective, a score was defined taken
angle of each point in the avoid chart, being this an effective measure
of how ``positive'' or ``negative'' a word is. If we call this angle
\(\theta\), we can generate an initial score that looks like the below
chart:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/theta_chart-1} \end{center}

Although this is a good approach, after several iterations the following
formula has been chosen instead:

\[pre\_score = sin(pos\_proportion)^{\lambda_1}*\frac{\theta}{90 \si{\degree} }\]

\[score = \frac{pre\_score}{max(pre\_score)} - \lambda_2\] where *
\(pos\_proportion\): Proportion of times where word was found in a
``positive'' tweet. This sinus of this parameter is used to help
penalise less relevant words. * \(\lambda_1\): Parametrisation
parameter, used to modulate the effect of less relevant words. *
\(\theta\): angle of vector
\((positive\_proportion,negative\_proportion)\), as discussed
previously. * \(\lambda_2\): Offset to potentially give a negative value
to less relevant words, in order to provide compensation - e.g.~in case
a ``disaster'' word is used in figurative context amongst many non
relevant words. This will be treated as a tuning parameter.

Setting \(\lambda_1\) to 1 and \(\lambda_2\), we have a sample
distribution, as follows:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/scores_sample_0-1} \end{center}

Looking at the chart above, we have achieve the objectives of giving
distinct scores depending on the apparent ``catastrophe'' value of each
word. This however, does not reflect the amount of times a word appears
- it may be the case that a particular term appears just one and gets an
extreme score which is not really a good indicator of its real
"catastrophe value. Thus, a last modification of the formula es
required:

\[score = 
 \begin{cases} 
      \emptyset & n \leq \lambda_3 \\
      \frac{pre\_score}{max(pre\_score)} - \lambda_2 & n > \lambda_3
   \end{cases}\]

where * \(n\) : number of times the word is appears in the training set,
* \(\lambda_3\): arbitrary theshold to filter word with a low number of
ocurrences (and possibly biased). This is a tuning parameter.

Using \(\lambda_3=10\) for illustration purposes, we have the below
chart:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/scores_sample_1-1} \end{center}

With a set of scores, we would like to know if this produce a set of
tweet scores that are a potential good input for a regression function.
Due its simplicity, if we using a sum to calculate a score per tweet we
can obtain those values with the below code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   tweet_score <-}\StringTok{ }\NormalTok{tokenised_words }\OperatorTok{%>%}\StringTok{ }
\StringTok{                 }\KeywordTok{left_join}\NormalTok{(word_scores,}\DataTypeTok{by=}\StringTok{"word"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(score)) }\OperatorTok{%>%}
\StringTok{                 }\KeywordTok{group_by}\NormalTok{(id) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{word_score=}\KeywordTok{sum}\NormalTok{(score)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                 }\KeywordTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

wich, will produce results like the below graphs (for different offset
values):

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/tweet_scores-1} \end{center}

From the above, we can see that there is a diffuse division between
positive and negative tweets. It is worth noticing this could
potentially be improved with better modelling - however this could a
problem but itself and the above - with tuning - is deemed acceptable
for a first approximation.

A similar process has been done two create tweet scores for hasthags,
links(or rather the domains of the unshorten URLs) and mentions. After
try and error, a slightly different formula has been chosen for these
three observations, represented below for hasthags:

\[pre\_score= \left( \frac{pos\_proportion}{\lambda_1} \right)^{\lambda_{2}} \cdot \frac{\theta}{90}\]
\[ score =
            \begin{cases}
            \emptyset & n \leq \lambda_4 \\
            \frac{pre\_score}{max(pre\_score)}  - \lambda_3 & n > \lambda_4 \\
            \end{cases}\]

where

\begin{itemize}
\tightlist
\item
  \(pos\_proportion\) is the proportion of the term in the entire corpus
  of positive hasthags/domains/mentions,
\item
  \(\lambda_1\) tuning parameter that re-scales - everything above the
  threshold is relevant, the rest not so much,
\item
  \(\lambda_2\) tuning parameter to weigh down irrelavent terms and
  weigh up relevent ones,
\item
  \(\theta\) angle of vector \((negatives,positives)\),
\item
  \(\lambda_3\), Offset to potentially give a negative value to less
  relevant term, in order to provide compensation - e.g.~in case a
  ``disaster'' word is used in figurative context amongst many non
  relevant term This will be treated as a tuning parameter.
\item
  \(\lambda_4\), threshold of number of ocurrences necessary to consider
  the score as valid.
\end{itemize}

The below charts show an example of score generated using this method:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/other_scores-1} \end{center}

In order to facilitate the next steps, the above process has been
written into three functions, namely:

\begin{itemize}
\tightlist
\item
  \textbf{tokenise\_data}, which takes the data in the pre-processed
  format and generates a dataframes with the token words, hasthags,
  domains and Twitter handles.
\item
  \textbf{calculate\_scores},which takes the output of the previous
  function (\emph{tokenised} data) and calculate the scores for each
  word, hashtag, link and handle. All the previously discussed tuning
  parameter have been defined as inputs for this function.
\item
  \textbf{score\_tweets}, which will generate a score for earch tweeet
  for each of categories previously mentioned.
\end{itemize}

All these functions are written in the accompanying .R file, with inline
commentary explaining all the steps involved in the process.

\hypertarget{finalising-the-model-and-data-quality.}{%
\subsection{Finalising the model and data
quality.}\label{finalising-the-model-and-data-quality.}}

From here, we can move to the next step and to generate the last part of
the model - the one that will render a prediction.

Considering the work done so far, there are potentially many ways to use
to the scores to create a model for prediction. After weighing the
options, the following two-step approach will used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, a regression algorithm will be used to train a model for each
  attribute - i.e.~4 different predictions will be generated using each
  of the 4 scores previously created independently. The result of this
  process will be vector for each tweet, with the results of each
  prediction (1,0 or potentially N/A if there is no such attribute for a
  given tweet.)
\item
  Then this vector will be used again as a input of a second machine
  learning method , which will attempt to predict the target result
  based on the individual responses.
\end{enumerate}

For this process, two functions have been written in this report .R
file:

\begin{itemize}
\tightlist
\item
  \textbf{fit\_model}, which is the training function. Its inputs are
  the scores plus relevant tuning parameters. It will output 5 machine
  learning models (for each attribute plus the aggregate one).
\item
  \textbf{predict\_values}, which will take the training score, the test
  data and the models previously generated. it will output the
  predictions and measure accuracy if the target value is provided.
\end{itemize}

These functions leverage the caret package and they have been written to
provide some flexibility - for example \textbf{fit\_model} allows to be
run using different machine learning models. For further details, please
refer to the detailed commentary in the .R file.

For the initial run - that will help us establish and initial benchmark
- the general linear model (\href{}{glm}) in caret is used for the
indidivual predictions. For the aggregate preidction, we will use
\href{https://en.wikipedia.org/wiki/XGBoost}{xgbBoost}. For this the
below code is used:

The accuracy for table and confusion matrix for this example are:

\begin{table}[H]

\caption{\label{tab:fp_acc}Initial results}
\centering
\begin{tabular}[t]{lr}
\toprule
Method & Accuracy\\
\midrule
\rowcolor{gray!6}  word & 0.8183538\\
hashtag & 0.8148148\\
\rowcolor{gray!6}  link & 0.8108108\\
handle & 0.8095238\\
\rowcolor{gray!6}  aggregate & 0.8188679\\
\bottomrule
\end{tabular}
\end{table}

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/confusion_matrix0-1} \end{center}

As observed, there are large number of false positives and false
negatives. A sample of those is provided below:

\begin{table}[H]

\caption{\label{tab:result_1_FP}Initial results - False Positives}
\centering
\begin{tabular}[t]{rr>{\raggedright\arraybackslash}p{30em}}
\toprule
\rowcolor{gray!6}  id & target & original\_text\\


\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]

\caption{\label{tab:result_1_FN}Initial results - False Negatives}
\centering
\begin{tabular}[t]{rr>{\raggedright\arraybackslash}p{30em}}
\toprule
\rowcolor{gray!6}  id & target & original\_text\\


\bottomrule
\end{tabular}
\end{table}

A more detailed review of both sets, seems to indicated that the source
file curation does seem to contain some errors : tweets that are
seemingly not a catastrophe are tagged as such and vicerversa. One
particular example of this are tweets allusive to the Hiroshima bombing
in World War 2. Although those tweets a tragedy remembrance and do not
carry the connotation of a current catastrophe, there are marked as
``disaster'' in the dataset.

\begin{table}[H]

\caption{\label{tab:hiroshima_tweets}Initial results - Hiroshima Tweets}
\centering
\begin{tabular}[t]{rr>{\raggedright\arraybackslash}p{30em}}
\toprule
id & target & original\_text\\
\midrule
1 535 & 1 & Hiroshima prepares to remember the day the bomb dropped http://t.co/oJHCGZXLSt\\
1 626 & 1 & 70 years ago today the United States of America bombed Hiroshima in Japan.\\
1 642 & 1 & Just Happened in Asia: Japan marks 70th anniversary of Hiroshima atomic bombing http://t.co/E5dEmdScEh\\
1 652 & 1 & 70th Anniversary of Hiroshima bombing https://t.co/juwvomhGPd\\
3 995 & 1 & I visited Hiroshima in 2006. It is an incredible place. This model shows devastation of the bomb. http://t.co/Gid6jqN8UG\\
\addlinespace
5 032 & 1 & How 'Little Boy' Affected the People In Hiroshima - Eyewitness Testimonials! - https://t.co/5x5hSV5sKO\\
5 041 & 1 & On anniversary of Hiroshima bombing illustrated timeline of bombings. Eyewitness account particularly horrifying http://t.co/GZIb0mAwmn\\
5 523 & 1 & 70 years ago the first atomic attack flattened \#Hiroshima 3 days later it was \#Nagasaki both war crimes to put Moscow in its place\\
7 129 & 1 & Courageous and honest analysis of need to use Atomic Bomb in 1945. \#Hiroshima70 Japanese military refused surrender. https://t.co/VhmtyTptGR\\
7 441 & 1 & The day Hiroshima through the eyes of the bomber crew and survivors http://t.co/AYEWJ8marn via @MailOnline\\
\addlinespace
7 490 & 1 & Here we are 70 years after the nuclear obliteration of Hiroshima and Nagasaki and Iâ€™m wondering iâ€\_ http://t.co/fvkekft4Rs\\
7 520 & 1 & Hannah: 'Hiroshima sounds like it could be a place in China. Isn't that where the oil spill was?'\\
9 397 & 1 & Hiroshima survivors fight nuclear industry in Brazil Ã¢?? video http://t.co/GLZmGBM7w0\\
9 442 & 1 & Remembrance  http://t.co/ii4EwE1QIr \#Hiroshima http://t.co/H3vUsqzyQo\\
\bottomrule
\end{tabular}
\end{table}

The above reflects two important problems with machine learning and
natural language:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  This re-enforces that quality training data is essential. Bad data
  will probably result in bad results - even with a very sophisticated
  algorithm (It's called \emph{Data} Science after all!).
\item
  Human language is complicated. Meaning of each word is highly
  contextual and changes depending on the register, dialect and over
  time. This can be difficult for a machine learning model to
  \textbf{grasp}.
\end{enumerate}

To illustrate the second point, the above false positives and false
negatives have been manually review and their new positive/negative
result has been modified accordingly and added into the original data
set. After feeding them back into the trainig dataset, selecting a new
testing sample and running the machine learning model again, the below
results are obtained:

\begin{table}[!h]

\caption{\label{tab:fp_acc0}Initial results - corrected dataset}
\centering
\begin{tabular}[t]{lr}
\toprule
Component & Accuracy\\
\midrule
\rowcolor{gray!6}  word & 0.7691580\\
hashtag & 0.7968750\\
\rowcolor{gray!6}  link & 0.7101449\\
handle & 0.7500000\\
\rowcolor{gray!6}  aggregate & 0.7688679\\
\bottomrule
\end{tabular}
\end{table}

This illustrates that better training data can lead to better results.
It is also worth noticing this may well reflect a real-life scenario,
where a social media monitoring centre is constantly reviewing false
negatives and false negatives with the purpose of keep re-training a
model to adapt to changes language, emerging events, etc.

Of course, the other components in the model can also benefit of this
\emph{curated} approach. In this report, and for illustraition process -
the list of hasthags in thre training set has been reviewed and then
detected a couple of hashtags that most probably mean catasthrophe
(e.g.~like \#earthquake most like means one) and other most probably
won't (like \#ashes2017, which is unmistakebly cricket, not matter how
colourful and hiperbolic is language).

Using a sample set of curated hashtag, it is possible to observe a small
increment in accuracy. Handles and links will be left as a action to
improve for the time being.

\begin{table}[!h]

\caption{\label{tab:fp_acc3}Initial results - Curated Hashtags}
\centering
\begin{tabular}[t]{lr}
\toprule
Component & Accuracy\\
\midrule
\rowcolor{gray!6}  word & 0.7738884\\
hashtag & 0.8181818\\
\rowcolor{gray!6}  link & 0.7101449\\
handle & 0.7500000\\
\rowcolor{gray!6}  aggregate & 0.7735849\\
\bottomrule
\end{tabular}
\end{table}

Looking at the work done so far, we have the below table showing the
improvements obtained by improving and supplementing the data:

\begin{table}[!h]

\caption{\label{tab:results_comparison_1}Initial Comparison}
\centering
\begin{tabular}[t]{lr}
\toprule
attempt & accuracy\\
\midrule
\rowcolor{gray!6}  Initial training & 0.8188679\\
Initial training - Data Correction & 0.7688679\\
\rowcolor{gray!6}  Curated hashtags & 0.7735849\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tuning}{%
\subsection{Tuning}\label{tuning}}

With this new, ``augmented'' dataset the next step of the process is to
tune the model. Looking at how the model it's constructed, there are
\emph{many} aspects to tune, namely:

\begin{itemize}
\tightlist
\item
  The parameters used to calculate the tweet, hasthag, link and mention
  scores.
\item
  The selection of classification algorithms used.
\item
  The tunning parameters of each selection algorithm.
\end{itemize}

Obviously, tuning every single parameter will result in quite a lengthy
excercise. Instead, this report will focus on some of the key components
in this model and discus what works and what else could be done.

The first aspect to tune is the each word in the training corpus gets a
indvidual score. This is perhaps the key component of the model. From
the previous section, we have that this score is calculated through the
below formulas:

\[pre\_score = sin(pos\_proportion)^{\lambda_1}*\frac{\theta}{90 \si{\degree} }\]
\[score = 
 \begin{cases} 
      \emptyset & n \leq \lambda_3 \\
      \frac{pre\_score}{max(pre\_score)} - \lambda_2 & n > \lambda_3
   \end{cases}\]

where * \(pos\_proportion\): Proportion of times where word was found in
a ``positive'' tweet. This sinus of this parameter is used to help
penalise less relevant words. * \(\lambda_1\): Parametrisation
parameter, used to modulate the effect of less relevant words. *
\(\theta\): angle of vector
\((positive\_proportion,negative\_proportion)\), as discussed
previously. * \(\lambda_2\): Offset to potentially give a negative value
to less relevant words, in order to provide compensation - e.g.~in case
a ``disaster'' word is used in figurative context amongst many non
relevant words. This will be treated as a tuning parameter. * \(n\) :
number of times the word is appears in the training set, *
\(\lambda_3\): arbitrary theshold to filter word with a low number of
ocurrences (and possibly biased). This is a tuning parameter.

In this report, \(\lambda_2\) and \(\lambda_3\) will be tuned, to see
how the penalisation of irrelevant words and minimum number of
ocurrences affect the model. The results of tuning lambda\_2 are shown
below.

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/word_lambda_2_tuning_results-1} \end{center}

As shown above, the best result is obtained with \(lambda_2\)=0.3. It is
also worth noticing that increasing this value won't necessarily result
in better results, since it will then penalise ``neutral'' terms too.

With this value, we will then also find if there is an optimal
\(\lambda_3\), i.e.~to see if removing the words that appear seldomly
have an negative impact in the model. The results are shown below:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/word_lambda_3_tuning_results-1} \end{center}

This result is a bit more interesting. As observed above, the first
optimal \(\lambda_3\) is quite a low value, however there is a relative
bigger decrease in accuracy around its neighbours when compared against
the second best \(\lambda_3\). It is important to notice that the
accuracy depends on the corpus of words in the test set and how they are
combined into sentences. This is of course random and it will change
from set to set. Even though we can do cross validation, perhaps it is
not a bad a idea (as an initial step), choose a perhaps sub-optimal
value for the sake of consistency - at least until more thorough tuning
is possible. Therefore, to continue with this excercise, a value of
\(lambda_3\)=10 has been chosen.

After the word scoring, perhaps the most relevant component in the model
is the scoring function for hashtags (given that hashtag intend to be
\textbf{meaningful} descriptors). Similarly to the word scoring, we will
tune the equivalent parameters hashtag's \(lamdba_2\) and \(lamdba_3\).

The results for the tuning of \(lamdba_2\) are shown below:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/hashtag_lambda_2_tuning_results-1} \end{center}

The first graph shows the overall accuracy results (comparable to the
word tuning), while the second graph shows the results where there is a
hasthag. From them, the following points can be made:

\begin{itemize}
\tightlist
\item
  Smaller variations in the overall score can be explained to the fact
  that not all tweets have hasthags, thus the effect of tuning this part
  of the model will obvisouly have a limited effect on the general
  result.
\item
  Taking the above point, it looks like its contribution to general
  score it is mostly when hasthags do indicate a cathastrophe (and
  therefore have a bigger score). It is worth noticing that many of the
  high-score hashtag were not ranked by the algorithm but manually added
  - which is perhaps and indication that manual curation is not
  necessarily a bad idea - especially when data is poor quality or
  insufficient.
\end{itemize}

With this mind, the tuning of \(lamdba_3\) will help to determine the
overall effect of hashtag occurence. The results are shown in the below
chart.

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/hashtag_lambda_3_tuning_results-1} \end{center}

This result seems similar in nature to the tuning for quantity cut-off
for the word scoring.

Looking at the results so far (table below), this tuning process has
only resulted in small gains when compared with the initial data
corrections. Taking that the effect of hasthag tuning is small, in this
report the tuning of the links and handle scoring functions will not be
conducted. This doesn't mean they may bring an improved accuracy but
perhaps are not worth the effort given the existing training dataset.

\begin{table}[H]

\caption{\label{tab:results_comparison_2}Initial Comparison}
\centering
\begin{tabular}[t]{lr}
\toprule
attempt & accuracy\\
\midrule
\rowcolor{gray!6}  Initial training & 0.8188679\\
Initial training - Data Correction & 0.7688679\\
\rowcolor{gray!6}  Curated hashtags & 0.7735849\\
Optimised word score - lambda\_2 & 0.7990566\\
\rowcolor{gray!6}  Optimised word score - lambda\_3 & 0.7990566\\
\addlinespace
Optimised hashtag score - lambda\_3 & 0.7989691\\
\bottomrule
\end{tabular}
\end{table}

Instead, the decission is to focus on the tuning of the classification
functions. At this point, the model uses the caret package to leverage
to known machine learning algorithms, namely:

\begin{itemize}
\tightlist
\item
  \emph{glm} (as in generalised linear model) in its logisitc regression
  mode to turn word, hashtag, link and mention to make a prediction on
  those components independently.
\item
  \emph{xgbTree} (extreme gradient boosting) to predict an ``overall''
  positive or negative based on the previous predictions.
\end{itemize}

Considering that the assignment instructions state \textbf{``For this
project, you will be applying machine learning techniques that go beyond
standard linear regression''}, the next stage will be to asses if there
is any model that produces better results than glm. (Although so far
this is not a \textbf{standard} linear regression model anymore.)

After investigating a number of possible algorithms, the
\href{https://en.wikipedia.org/wiki/LogitBoost}{\textbf{LogitBoost}}
will use as comparison. This algorithm is available in R through the
\href{https://www.rdocumentation.org/packages/caTools/versions/1.17.1/topics/LogitBoost}{\textbf{caTools}}
package.

As a starting point, we will try this algorithm for the word score
classification function. The below results show the best result after
tuning its only parameter, which is the number of iterations.

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/word_logitboost_result-1} \end{center}

Please note graph above shows the accuracy results for the LogitBoost
regression only, not for the entire model. However, it shows that in
this case the tuning has not effect. In this case, the overall accuracy
equals \emph{0.795}.These result is no better than the one using the a
simple logistic regression. Similar results are obtained when the same
comparison is repeated for the other scoring components.

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/other_logitboost_result-1} \end{center}

In this case, the accuracy value equals \textbf{0.795}, which is no
better than the classification using a logistic regresion.

This results shouldn't be confusing - a more ``sophisticated'' method
does not necessarily mean a better result the important thing is to find
the model that better represents the problem at hand (when possible).
Simpler models may also have an advantage in terms of processing time -
which may be a key factor to consider even if other models perform
marginally better.

Completed the above steps, there is one aspect left to tune : the
XGBoost algorithm. This method has several parameters, for which a good
explanation of them can be found
\href{https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/}{here}.

Given the number of parameters, a full sweep of all them could take
very, very long. However, the caret packages already does some tuning by
default. We will use the results from the previous iteration as a
starting point.

\begin{table}[H]

\caption{\label{tab:xgbTree_best_tune}XGBoost - Best Tune}
\centering
\begin{tabular}[t]{lrrrrrrr}
\toprule
  & nrounds & max\_depth & eta & gamma & colsample\_bytree & min\_child\_weight & subsample\\
\midrule
\rowcolor{gray!6}  13 & 50 & 1 & 0.3 & 0 & 0.8 & 1 & 0.75\\
\bottomrule
\end{tabular}
\end{table}

Below is the code used for tuning:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training_tokenised <-}\StringTok{ }\KeywordTok{tokenise_data}\NormalTok{(training,extra_stop_words,}
\NormalTok{                                    domains_data,}\DataTypeTok{training_flag=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{scores<-}\StringTok{ }\KeywordTok{calculate_scores}\NormalTok{(training_tokenised,}
                             \DataTypeTok{word_parameters=}\KeywordTok{c}\NormalTok{(}\StringTok{"sin"}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}
\NormalTok{                                               word_lambda_}\DecValTok{3}\NormalTok{,word_lambda_}\DecValTok{2}\NormalTok{),}
                             \DataTypeTok{hashtag_parameters=}\KeywordTok{c}\NormalTok{(}\StringTok{"exp"}\NormalTok{,}\DecValTok{4}\NormalTok{,}
\NormalTok{                                                  hashtag_lambda_}\DecValTok{2}\NormalTok{,hashtag_lambda_}\DecValTok{3}\NormalTok{),}
                             \DataTypeTok{handle_parameters=}\KeywordTok{c}\NormalTok{(}\StringTok{"exp"}\NormalTok{,}\DecValTok{4}\NormalTok{,}\FloatTok{0.8}\NormalTok{,}\DecValTok{4}\NormalTok{),}
                             \DataTypeTok{link_parameters=}\KeywordTok{c}\NormalTok{(}\StringTok{"exp"}\NormalTok{,}\DecValTok{4}\NormalTok{,}\FloatTok{0.9}\NormalTok{,}\DecValTok{4}\NormalTok{),}
                              \DataTypeTok{manual_scores =}\NormalTok{ manual_scores)}
\NormalTok{training_vector <-}\StringTok{ }\KeywordTok{score_tweets}\NormalTok{(training,training_tokenised,scores) }
 
\NormalTok{aggregate_tuning_grid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{nrounds =} \KeywordTok{seq}\NormalTok{(}\DecValTok{45}\NormalTok{,}\DecValTok{55}\NormalTok{,}\DecValTok{1}\NormalTok{),  }
                                     \DataTypeTok{max_depth =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{,}
                                     \DataTypeTok{eta =} \KeywordTok{seq}\NormalTok{(}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.1}\NormalTok{), }
                                     \DataTypeTok{gamma =}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.1}\NormalTok{),}
                                     \DataTypeTok{colsample_bytree =} \KeywordTok{seq}\NormalTok{(}\FloatTok{0.7}\NormalTok{,}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.1}\NormalTok{), }
                                     \DataTypeTok{min_child_weight =} \DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }
                                     \DataTypeTok{subsample =} \FloatTok{0.75}\NormalTok{) }

\NormalTok{fitting_model <-}\StringTok{ }\KeywordTok{fit_model}\NormalTok{(training_vector,}\DataTypeTok{method_word=}\StringTok{"glm"}\NormalTok{,}
                      \DataTypeTok{method_hashtag=}\StringTok{"glm"}\NormalTok{, }
                      \DataTypeTok{method_link=}\StringTok{"glm"}\NormalTok{, }
                      \DataTypeTok{method_handle=}\StringTok{"glm"}\NormalTok{, }
                      \DataTypeTok{method_aggregate=}\StringTok{"xgbTree"}\NormalTok{,}
                      \DataTypeTok{tuneGrid_aggregate =}\NormalTok{ aggregate_tuning_grid)}

\NormalTok{results<-}\KeywordTok{predict_values}\NormalTok{(test,fitting_model,scores,extra_stop_words,domains_data)}

\NormalTok{aggregate_model_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\NormalTok{fitting_model}\OperatorTok{$}\NormalTok{aggregate}
\NormalTok{stats_}\DecValTok{7}\NormalTok{ <-}\StringTok{ }\NormalTok{results}\OperatorTok{$}\NormalTok{eval}
\end{Highlighting}
\end{Shaded}

The optimised solution results in an accuracy of \textbf{0.799}. The
optimised XGBoost parameters are:

\begin{table}[H]

\caption{\label{tab:xgbTree_best_tune2}XGBoost - Best Tune}
\centering
\begin{tabular}[t]{lrrrrrrr}
\toprule
  & nrounds & max\_depth & eta & gamma & colsample\_bytree & min\_child\_weight & subsample\\
\midrule
\rowcolor{gray!6}  254 & 45 & 1 & 0.2 & 0.2 & 0.8 & 3 & 0.75\\
\bottomrule
\end{tabular}
\end{table}

When compared with all the previous results, we have the below chart

Initial Comparison

attempt

accuracy

Initial training

0.8188679

Initial training - Data Correction

0.7688679

Curated hashtags

0.7735849

Optimised word score - lambda\_2

0.7990566

Optimised word score - lambda\_3

0.7990566

Optimised hashtag score - lambda\_3

0.7989691

Word score - LogitBoost

0.7952830

Other scores - LogitBoost

0.7989691

Optimised XGBoost

0.7990566

As shown above, the greatest gains in accuracy are due to the data
correction and the tuning of the word scoring formula. Further tuning
only produces worse or marginally better results (from hashtag
optimisation to optimised XGBoost, for example). Furthermore, the first
impresion is that perhaps there is a ceiling that has been reached,
perhaps due to the limitation of the model or quality of the training
data (bad inputs -\textgreater{} bad results).

Nevertheless, up to this point we have try touched all the relevant
aspects of the model. Therefore (for the purpose of the this
assignment), the only step left is to assess against the validation data
and then comment based on those results.

\hypertarget{results}{%
\section{Results}\label{results}}

As the last step of this report, the model has been assessed against the
\textbf{validation} dataset that was set aside a the beginning of this
excercise. In order to acomplish this, the below steps have been
followed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Training} and \textbf{test} set have been merged into one
  dataset.
\item
  The model has been trained against this new dataset.
\item
  The newly-trained model has been used to generate prediction for the
  \textbf{validation} dataset
\end{enumerate}

The result of this is displayed in the below accuracy summary

\begin{table}[H]

\caption{\label{tab:validation_results}XGBoost - Best Tune}
\centering
\begin{tabular}[t]{lr}
\toprule
Component & Accuracy\\
\midrule
\rowcolor{gray!6}  word & 0.7994324\\
hashtag & 0.7979798\\
\rowcolor{gray!6}  link & 0.7536232\\
handle & 0.8750000\\
\rowcolor{gray!6}  aggregate & 0.7990566\\
\bottomrule
\end{tabular}
\end{table}

The confusion matrix for this result is shown below:

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/validation_cm-1} \end{center}

Looking at those results, it looks like we have reached some sort of
ceiling for the model. As mentioned before, possible culprits are the
model or the data. Considering that at the start of the this report data
issues were flagged, it wouldn't be unreasonable to suspect that is a
main contributor. To confirm this, both false positives and false
negatives were inspected - below are a sample of both groups:

\begin{table}[H]

\caption{\label{tab:validation_FP}Validation Results - False Positives}
\centering
\begin{tabular}[t]{rr>{\raggedright\arraybackslash}p{30em}}
\toprule
id & target & original\_text\\
\midrule
\rowcolor{gray!6}  490 & 0 & 9 Charts Prove Financial Crisis Part 2 Has BEGUN!: The Financial Armageddon Economic Collapse Blog tracks tren... http://t.co/vHCXTvCINr\\
650 & 0 & Credit to @pfannebeckers for inspiring me to rediscover this fantabulous \#tbt http://t.co/wMHy47xkiL\\
\rowcolor{gray!6}  2 153 & 0 & @BarackObama hello Mr. President there is a really big problem in Puerto Rico regarding the water situation no more like a catastrophe...\\
5 434 & 0 & Attention Service Members Veterans Educators First Responders in Jacksonville FL http://t.co/4UrtBEAcE5\\
6 486 & 0 & Injuries may be forgiven but not forgotten.

\rowcolor{gray!6}  Aesop\\
\addlinespace
9 600 & 0 & Raise your words not your voice. It is rain that grows flowers not thunder.\\
\rowcolor{gray!6}  10 402 & 0 & NEW YORK: A whirlwind day of activities in New York. Breakfast at the Millennium Hotel United Nations Plaza. Lunch... http://t.co/laYZBA9y8h\\
10 492 & 0 & 11:57am Wildfire by The Mynabirds from Lovers Know\\
\bottomrule
\end{tabular}
\end{table}

After inspecting for set, it looks like there is a problem of
misclassification with the false negatives. In this case the model
\emph{correctly} predicts most of them as not a disaster tweet. However,
upon inspection it seems the model is clearly \emph{incorrectly}
classifying most of all false positives. It is worth noticing though
that all of them contain ``catasthrophe'' words. A possible reason of
this that the model is mostly driven by the words rather than the other
attributes and its limited lexicon results in wrong scores for those
tweets. Fortunately, it is possible to see for those tweets whether a
prediction for each component was generated of not, which is shown in
the below charts, for false positives and all validation tweets.

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/validation_predictions_fp-1} \end{center}

Both results show - interestingly that the overall results is almost
entirely driven by the word scoring model. In fact, this model is
basically ignoring the other attributes. This can be explained by the
attribute distribution in the training data. As shown in below graph,
the tweets that only have words exceed by far any other combination,
which probably results in the XGBoost algorithm ignoring all other
components. Although in theory hashtags, links and handles make sense,
the existing data is making them irrelevant.

This probably only compounds another problem previously mentioned: the
absence of several words in the lexicon created by the training data.
This is summarised by the below chart.

\begin{center}\includegraphics{Twitter_Classifier_files/figure-latex/tokenise_validation-1} \end{center}

The consequence of this will be incorrect or incomplete scoring - this
is one probable reason for the high number of false negatives. Again,
the conclusion seems to be that good quality training data, in
reasonable quantities is critical for a good machine learning model.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\begin{itemize}
\tightlist
\item
  Good quality data matters. Bad input means bad results.
\item
  Sometimes manual curation will improve the results.
\item
  There is so much to tune.
\end{itemize}

\end{document}
